/**
 * @fileoverview Chat Memory Eligibility Processor
 * –Ü–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î —á–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –¥–æ–≤–≥–æ—Ç—Ä–∏–≤–∞–ª—É –ø–∞–º'—è—Ç—å –≤ Chat —Ä–µ–∂–∏–º—ñ
 * 
 * BEST PRACTICES:
 * - Semantic analysis –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è memory triggers
 * - Context-aware decision making
 * - Minimal latency (fast LLM –∞–±–æ rule-based)
 * - Smart caching –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –ø–æ–≤—Ç–æ—Ä–Ω–∏—Ö –≤–∏–∫–ª–∏–∫—ñ–≤
 * 
 * @version 1.0.0
 * @date 2025-10-26
 */

import logger from '../../utils/logger.js';
import axios from 'axios';
import GlobalConfig from '../../../config/atlas-config.js';

/**
 * Chat Memory Eligibility Processor
 * –í–∏–∑–Ω–∞—á–∞—î —á–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏ –¥–æ–≤–≥–æ—Ç—Ä–∏–≤–∞–ª—É –ø–∞–º'—è—Ç—å –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –≤ Chat —Ä–µ–∂–∏–º—ñ
 */
export class ChatMemoryEligibilityProcessor {
    /**
     * @param {Object} dependencies
     * @param {Object} dependencies.logger - Logger instance
     * @param {Object} dependencies.mcpManager - MCP Manager –¥–ª—è –¥–æ—Å—Ç—É–ø—É –¥–æ Memory Server
     */
    constructor({ logger: loggerInstance, mcpManager }) {
        this.logger = loggerInstance || logger;
        this.mcpManager = mcpManager;
        
        // Cache –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –ø–æ–≤—Ç–æ—Ä–Ω–∏—Ö LLM –≤–∏–∫–ª–∏–∫—ñ–≤
        this.decisionCache = new Map();
        this.cacheTimeout = 60000; // 1 —Ö–≤–∏–ª–∏–Ω–∞
        
        // Statistics –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É
        this.stats = {
            totalRequests: 0,
            memoryUsed: 0,
            memorySkipped: 0,
            cacheHits: 0,
            averageDecisionTime: 0
        };
    }

    /**
     * –í–∏–∫–æ–Ω–∞—Ç–∏ –∞–Ω–∞–ª—ñ–∑ —á–∏ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –¥–æ–≤–≥–æ—Ç—Ä–∏–≤–∞–ª–∞ –ø–∞–º'—è—Ç—å
     * 
     * @param {Object} context - Context
     * @param {string} context.userMessage - User message
     * @param {Object} context.session - Session context
     * @param {Array} context.recentMessages - Recent conversation history
     * @returns {Promise<Object>} Decision result
     */
    async execute(context) {
        const startTime = Date.now();
        this.stats.totalRequests++;

        const { userMessage, session, recentMessages = [] } = context;

        this.logger.system('chat-memory', '[MEMORY-ELIGIBILITY] üß† Analyzing if long-term memory is needed...');

        try {
            // LAYER 1: Rule-based fast checks (0-1ms)
            const ruleBasedDecision = this._ruleBasedAnalysis(userMessage, recentMessages);
            
            if (ruleBasedDecision.confidence >= 0.9) {
                // High confidence rule-based decision - skip LLM
                this.logger.system('chat-memory', `[MEMORY-ELIGIBILITY] ‚úÖ Rule-based decision: ${ruleBasedDecision.needsMemory ? 'USE' : 'SKIP'} memory (confidence: ${ruleBasedDecision.confidence})`);
                
                this._updateStats(startTime, ruleBasedDecision.needsMemory);
                
                return {
                    success: true,
                    needsMemory: ruleBasedDecision.needsMemory,
                    confidence: ruleBasedDecision.confidence,
                    reasoning: ruleBasedDecision.reasoning,
                    method: 'rule-based',
                    triggers: ruleBasedDecision.triggers,
                    decisionTime: Date.now() - startTime
                };
            }

            // LAYER 2: Cache check (1-2ms)
            const cacheKey = this._generateCacheKey(userMessage, recentMessages);
            const cachedDecision = this._getCachedDecision(cacheKey);
            
            if (cachedDecision) {
                this.stats.cacheHits++;
                this.logger.system('chat-memory', `[MEMORY-ELIGIBILITY] üíæ Cache hit - using cached decision`);
                
                this._updateStats(startTime, cachedDecision.needsMemory);
                
                return {
                    ...cachedDecision,
                    method: 'cached',
                    decisionTime: Date.now() - startTime
                };
            }

            // LAYER 3: LLM-based intelligent analysis (100-500ms)
            const llmDecision = await this._llmBasedAnalysis(userMessage, recentMessages, ruleBasedDecision);
            
            // Cache the decision
            this._cacheDecision(cacheKey, llmDecision);
            
            this._updateStats(startTime, llmDecision.needsMemory);
            
            this.logger.system('chat-memory', `[MEMORY-ELIGIBILITY] ü§ñ LLM decision: ${llmDecision.needsMemory ? 'USE' : 'SKIP'} memory (confidence: ${llmDecision.confidence})`);
            
            return {
                success: true,
                ...llmDecision,
                method: 'llm',
                decisionTime: Date.now() - startTime
            };

        } catch (error) {
            this.logger.error(`[MEMORY-ELIGIBILITY] ‚ùå Analysis failed: ${error.message}`);
            
            // Fallback: skip memory on error (safer)
            return {
                success: true,
                needsMemory: false,
                confidence: 0.5,
                reasoning: 'Error in analysis, defaulting to skip memory',
                method: 'fallback',
                error: error.message,
                decisionTime: Date.now() - startTime
            };
        }
    }

    /**
     * LAYER 1: Rule-based analysis (—à–≤–∏–¥–∫–∏–π, –±–µ–∑ LLM)
     * @private
     */
    _ruleBasedAnalysis(userMessage, recentMessages) {
        const messageLower = userMessage.toLowerCase();
        const triggers = [];
        let score = 0;

        // EXPLICIT MEMORY TRIGGERS (confidence 95%)
        const explicitTriggers = [
            { pattern: /–ø–∞–º'—è—Ç–∞—î—à|–ø–∞–º—è—Ç–∞—î—à|remember|recall/i, weight: 0.95, name: 'explicit_remember' },
            { pattern: /–º–∏–Ω—É–ª–æ–≥–æ —Ä–∞–∑—É|last time|previously|—Ä–∞–Ω—ñ—à–µ/i, weight: 0.9, name: 'reference_past' },
            { pattern: /—Ç–∏ –∫–∞–∑–∞–≤|—Ç–∏ –≥–æ–≤–æ—Ä–∏–≤|you said|you told/i, weight: 0.9, name: 'reference_previous_statement' },
            { pattern: /–Ω–∞—à–∞ —Ä–æ–∑–º–æ–≤–∞|our conversation|–º–∏ –æ–±–≥–æ–≤–æ—Ä—é–≤–∞–ª–∏|we discussed/i, weight: 0.85, name: 'reference_conversation' }
        ];

        for (const trigger of explicitTriggers) {
            if (trigger.pattern.test(messageLower)) {
                triggers.push(trigger.name);
                score = Math.max(score, trigger.weight);
            }
        }

        // IMPLICIT MEMORY TRIGGERS (confidence 70-80%)
        const implicitTriggers = [
            { pattern: /–ø—Ä–æ—î–∫—Ç|project|—Å–∏—Å—Ç–µ–º–∞|system|–∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞|architecture/i, weight: 0.75, name: 'project_context' },
            { pattern: /–Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è|settings|preferences|–≤–ø–æ–¥–æ–±–∞–Ω–Ω—è/i, weight: 0.7, name: 'preferences' },
            { pattern: /—è–∫ –∑–∞–≤–∂–¥–∏|as usual|–∑–≤–∏—á–∞–π–Ω–æ|typically/i, weight: 0.7, name: 'habitual_reference' },
            { pattern: /–ø—Ä–æ–¥–æ–≤–∂—É–π|continue|–¥–∞–ª—ñ|next/i, weight: 0.65, name: 'continuation' }
        ];

        for (const trigger of implicitTriggers) {
            if (trigger.pattern.test(messageLower)) {
                triggers.push(trigger.name);
                score = Math.max(score, trigger.weight);
            }
        }

        // SKIP MEMORY TRIGGERS (confidence 95%)
        const skipTriggers = [
            { pattern: /^(–ø—Ä–∏–≤—ñ—Ç|hi|hello|hey|–¥–æ–±—Ä–∏–π –¥–µ–Ω—å)/i, weight: -0.95, name: 'greeting' },
            { pattern: /^(–¥—è–∫—É—é|thanks|thank you|—Å–ø–∞—Å–∏–±—ñ)/i, weight: -0.9, name: 'gratitude' },
            { pattern: /^(—Ç–∞–∫|yes|–Ω—ñ|no|ok|okay|–¥–æ–±—Ä–µ)/i, weight: -0.85, name: 'simple_affirmation' },
            { pattern: /—â–æ —Ç–∞–∫–µ|what is|—Ö—Ç–æ —Ç–∞–∫–∏–π|who is|—è–∫ –ø—Ä–∞—Ü—é—î|how does/i, weight: -0.7, name: 'general_question' }
        ];

        for (const trigger of skipTriggers) {
            if (trigger.pattern.test(messageLower)) {
                triggers.push(trigger.name);
                score = Math.min(score, trigger.weight);
            }
        }

        // CONTEXT ANALYSIS: Check recent messages
        if (recentMessages.length > 2) {
            const hasOngoingConversation = recentMessages.slice(-3).some(msg => 
                msg.content && msg.content.length > 50
            );
            
            if (hasOngoingConversation && score > 0) {
                score += 0.1; // Boost if ongoing conversation
                triggers.push('ongoing_conversation');
            }
        }

        // Normalize score to confidence
        const confidence = Math.abs(score);
        const needsMemory = score > 0;

        return {
            needsMemory,
            confidence,
            triggers,
            reasoning: needsMemory 
                ? `Detected memory triggers: ${triggers.join(', ')}`
                : `No memory needed - simple ${triggers[0] || 'chat'}`
        };
    }

    /**
     * LAYER 3: LLM-based intelligent analysis
     * @private
     */
    async _llmBasedAnalysis(userMessage, recentMessages, ruleBasedHint) {
        const prompt = this._buildMemoryEligibilityPrompt(userMessage, recentMessages, ruleBasedHint);
        
        // Use chat_memory_eligibility model (ultra fast AI21 Jamba 1.5 Mini)
        const modelConfig = GlobalConfig.MCP_MODEL_CONFIG?.getStageConfig('chat_memory_eligibility') || {
            model: 'atlas-ai21-jamba-1.5-mini',
            temperature: 0.1,
            max_tokens: 150
        };

        const apiEndpoint = GlobalConfig.MCP_MODEL_CONFIG?.apiEndpoint?.primary || 
                           'http://localhost:4000/v1/chat/completions';

        try {
            const response = await axios.post(apiEndpoint, {
                model: modelConfig.model,
                messages: [
                    { role: 'system', content: prompt.system },
                    { role: 'user', content: prompt.user }
                ],
                temperature: modelConfig.temperature,
                max_tokens: modelConfig.max_tokens
            }, {
                timeout: 5000 // 5 second timeout for fast decision
            });

            const rawResponse = response.data.choices[0].message.content;
            return this._parseMemoryDecision(rawResponse);

        } catch (error) {
            this.logger.warn('chat-memory', `[MEMORY-ELIGIBILITY] LLM analysis failed: ${error.message}`);
            
            // Fallback to rule-based decision
            return {
                needsMemory: ruleBasedHint.needsMemory,
                confidence: Math.max(ruleBasedHint.confidence, 0.6),
                reasoning: `LLM failed, using rule-based decision: ${ruleBasedHint.reasoning}`,
                triggers: ruleBasedHint.triggers
            };
        }
    }

    /**
     * Build prompt for memory eligibility LLM
     * @private
     */
    _buildMemoryEligibilityPrompt(userMessage, recentMessages, ruleBasedHint) {
        const conversationContext = recentMessages.slice(-3).map(msg => 
            `${msg.role}: ${msg.content.substring(0, 100)}`
        ).join('\n');

        const system = `You are a memory eligibility analyzer. Determine if long-term memory is needed to answer the user's message.

RESPOND ONLY WITH JSON:
{"needs_memory": true/false, "confidence": 0.0-1.0, "reasoning": "brief explanation"}

USE MEMORY when:
- User references past conversations ("remember", "last time", "you said")
- User asks about preferences, settings, or project context
- User continues previous topic requiring context
- User asks "who am I" or personal questions

SKIP MEMORY when:
- Simple greetings (hi, hello, thanks)
- General knowledge questions (what is X, how does Y work)
- Simple affirmations (yes, no, ok)
- New unrelated topics

Rule-based hint: ${ruleBasedHint.reasoning}`;

        const user = `Recent conversation:
${conversationContext}

Current message: "${userMessage}"

Does this require long-term memory? Respond with JSON only.`;

        return { system, user };
    }

    /**
     * Parse LLM memory decision
     * @private
     */
    _parseMemoryDecision(rawResponse) {
        try {
            // Clean markdown
            let clean = rawResponse.trim()
                .replace(/^```json\s*/i, '')
                .replace(/^```\s*/i, '')
                .replace(/\s*```$/i, '')
                .trim();

            const parsed = JSON.parse(clean);

            return {
                needsMemory: !!parsed.needs_memory,
                confidence: typeof parsed.confidence === 'number' ? parsed.confidence : 0.7,
                reasoning: parsed.reasoning || 'LLM decision',
                triggers: parsed.triggers || []
            };

        } catch (error) {
            this.logger.warn('chat-memory', `Failed to parse LLM response: ${error.message}`);
            
            // Fallback parsing
            const needsMemory = rawResponse.toLowerCase().includes('true') || 
                               rawResponse.toLowerCase().includes('needs_memory');
            
            return {
                needsMemory,
                confidence: 0.6,
                reasoning: 'Fallback parsing',
                triggers: []
            };
        }
    }

    /**
     * Generate cache key
     * @private
     */
    _generateCacheKey(userMessage, recentMessages) {
        // Simple hash based on message content and recent context
        const contextHash = recentMessages.slice(-2).map(m => m.content?.substring(0, 20)).join('|');
        return `${userMessage.substring(0, 50)}:${contextHash}`;
    }

    /**
     * Get cached decision
     * @private
     */
    _getCachedDecision(cacheKey) {
        const cached = this.decisionCache.get(cacheKey);
        
        if (!cached) return null;
        
        // Check if expired
        if (Date.now() - cached.timestamp > this.cacheTimeout) {
            this.decisionCache.delete(cacheKey);
            return null;
        }
        
        return cached.decision;
    }

    /**
     * Cache decision
     * @private
     */
    _cacheDecision(cacheKey, decision) {
        this.decisionCache.set(cacheKey, {
            decision,
            timestamp: Date.now()
        });

        // Cleanup old cache entries (keep last 50)
        if (this.decisionCache.size > 50) {
            const firstKey = this.decisionCache.keys().next().value;
            this.decisionCache.delete(firstKey);
        }
    }

    /**
     * Update statistics
     * @private
     */
    _updateStats(startTime, needsMemory) {
        const decisionTime = Date.now() - startTime;
        
        if (needsMemory) {
            this.stats.memoryUsed++;
        } else {
            this.stats.memorySkipped++;
        }

        // Update average decision time
        this.stats.averageDecisionTime = 
            (this.stats.averageDecisionTime * (this.stats.totalRequests - 1) + decisionTime) / 
            this.stats.totalRequests;
    }

    /**
     * Get statistics
     */
    getStats() {
        return {
            ...this.stats,
            cacheSize: this.decisionCache.size,
            cacheHitRate: this.stats.totalRequests > 0 
                ? (this.stats.cacheHits / this.stats.totalRequests * 100).toFixed(2) + '%'
                : '0%',
            memoryUsageRate: this.stats.totalRequests > 0
                ? (this.stats.memoryUsed / this.stats.totalRequests * 100).toFixed(2) + '%'
                : '0%'
        };
    }
}

export default ChatMemoryEligibilityProcessor;
