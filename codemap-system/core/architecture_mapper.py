#!/usr/bin/env python3
"""
Architecture Mapper v2.0 - Ğ“Ğ»Ğ¸Ğ±Ğ¾ĞºĞ° Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¸
Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ· Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ², Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹, ÑÑ‚Ğ°Ñ‚ÑƒÑÑ–Ğ² Ñ‚Ğ° Ğ²Ğ·Ğ°Ñ”Ğ¼Ğ¾Ğ´Ñ–Ğ¹
"""

import json
import os
import ast
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any
from datetime import datetime
from collections import defaultdict
from dotenv import load_dotenv
import logging

# ĞĞ°Ğ»Ğ°ÑˆÑ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ»Ğ¾Ğ³ÑƒĞ²Ğ°Ğ½Ğ½Ñ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class FileStatus:
    """Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ñ„Ğ°Ğ¹Ğ»Ñƒ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ñ–"""
    ACTIVE = "ACTIVE"
    DEPRECATED = "DEPRECATED"
    UNUSED = "UNUSED"
    IN_DEVELOPMENT = "IN_DEVELOPMENT"
    LEGACY = "LEGACY"
    EXPERIMENTAL = "EXPERIMENTAL"
    BROKEN = "BROKEN"


class ArchitectureMapper:
    """Ğ“Ğ»Ğ¸Ğ±Ğ¾ĞºĞ° Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¸ Ğ· Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¸Ğ¼ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ğ¾Ğ¼"""
    
    def __init__(self, project_root: Optional[Path] = None):
        if project_root is None:
            env_path = Path(__file__).parent.parent / '.env.architecture'
            if env_path.exists():
                load_dotenv(env_path)
            
            project_root_str = os.environ.get('PROJECT_ROOT', '..')
            if not os.path.isabs(project_root_str):
                project_root = Path(__file__).parent.parent / project_root_str
            else:
                project_root = Path(project_root_str)
        
        self.project_root = Path(project_root).resolve()
        self.analysis_root = self.project_root
        
        # ĞšĞµÑˆ Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ²
        self.files_cache: Dict[str, Dict[str, Any]] = {}
        self.dependencies: Dict[str, Set[str]] = defaultdict(set)
        self.reverse_dependencies: Dict[str, Set[str]] = defaultdict(set)
        self.file_status: Dict[str, str] = {}
        self.last_modified: Dict[str, datetime] = {}
        self.broken_files: Dict[str, str] = {}
        
        # ĞšĞ¾Ğ½Ñ„Ñ–Ğ³
        self.max_depth = int(os.environ.get('MAX_ANALYSIS_DEPTH', 5))
        self.min_file_size = int(os.environ.get('MIN_FILE_SIZE', 100))
        self.max_file_size = int(os.environ.get('MAX_FILE_SIZE', 1000000))
        self.deprecated_threshold_days = int(os.environ.get('DEPRECATED_THRESHOLD_DAYS', 90))
    
    def analyze_architecture(self, max_depth: Optional[int] = None) -> Dict[str, Any]:
        """ĞĞ½Ğ°Ğ»Ñ–Ğ·ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¸"""
        if max_depth is None:
            max_depth = self.max_depth
        
        logger.info(f"ğŸ” ĞĞ½Ğ°Ğ»Ñ–Ğ· Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¸ Ğ½Ğ° Ğ³Ğ»Ğ¸Ğ±Ğ¸Ğ½Ñƒ {max_depth}...")
        
        # Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ²ÑÑ– Ñ„Ğ°Ğ¹Ğ»Ğ¸
        workflow_files = self._find_workflow_files()
        logger.info(f"   ğŸ“ Ğ—Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(workflow_files)} Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ²")
        
        # ĞĞ½Ğ°Ğ»Ñ–Ğ·ÑƒÑ”Ğ¼Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¸
        for file_path in workflow_files:
            self._analyze_file(file_path, depth=0, max_depth=max_depth)
        
        # Ğ’Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ”Ğ¼Ğ¾ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ¸
        self._determine_file_status()
        
        # Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ñ†Ğ¸ĞºĞ»Ñ–Ñ‡Ğ½Ñ– Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ñ–
        cycles = self._detect_circular_dependencies()
        logger.info(f"   ğŸ”„ Ğ—Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(cycles)} Ñ†Ğ¸ĞºĞ»Ñ–Ñ‡Ğ½Ğ¸Ñ… Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹")
        
        # Ğ‘ÑƒĞ´ÑƒÑ”Ğ¼Ğ¾ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñƒ ĞºĞ°Ñ€Ñ‚Ñƒ
        architecture = self._build_architecture_map(max_depth, cycles)
        
        logger.info("âœ… ĞĞ½Ğ°Ğ»Ñ–Ğ· Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾")
        return architecture
    
    def _find_workflow_files(self) -> List[Path]:
        """Ğ—Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ²ÑÑ– Ñ„Ğ°Ğ¹Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñƒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ"""
        files = []
        extensions = {'.js', '.ts', '.jsx', '.tsx', '.py', '.java', '.cpp', '.go'}
        exclude_dirs = {
            'node_modules', '__pycache__', '.git', '.venv', 'dist', 'build',
            'archive', '.archive', 'backups', '.cache', '.idx', '.vscode',
            '.DS_Store', '.pytest_cache', 'venv', 'env', 'docs', 'logs', 'reports',
            'codemap-system'
        }
        
        for file_path in self.analysis_root.rglob('*'):
            if file_path.is_dir():
                continue
            
            # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ”Ğ¼Ğ¾ Ğ¿Ñ€Ğ¸Ñ…Ğ¾Ğ²Ğ°Ğ½Ñ– Ñ„Ğ°Ğ¹Ğ»Ğ¸
            if any(part.startswith('.') for part in file_path.parts):
                continue
            
            # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ”Ğ¼Ğ¾ Ğ²Ğ¸ĞºĞ»ÑÑ‡ĞµĞ½Ñ– Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ñ–Ñ—
            if any(part in exclude_dirs for part in file_path.parts):
                continue
            
            # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ÑÑ”Ğ¼Ğ¾ Ñ€Ğ¾Ğ·ÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ
            if file_path.suffix in extensions:
                # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ÑÑ”Ğ¼Ğ¾ Ñ€Ğ¾Ğ·Ğ¼Ñ–Ñ€
                try:
                    size = file_path.stat().st_size
                    if self.min_file_size <= size <= self.max_file_size:
                        files.append(file_path)
                except:
                    pass
        
        return sorted(files)
    
    def _analyze_file(self, file_path: Path, depth: int = 0, max_depth: int = 5):
        """ĞĞ½Ğ°Ğ»Ñ–Ğ·ÑƒĞ²Ğ°Ñ‚Ğ¸ Ñ„Ğ°Ğ¹Ğ» Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾"""
        if depth > max_depth:
            return
        
        file_key = str(file_path.relative_to(self.project_root))
        
        if file_key in self.files_cache:
            return
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            logger.warning(f"Error reading {file_path}: {e}")
            return
        
        # Ğ’Ğ¸Ñ‚ÑĞ³ÑƒÑ”Ğ¼Ğ¾ Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ñ Ğ¿Ñ€Ğ¾ Ñ„Ğ°Ğ¹Ğ»
        file_info = {
            'path': file_key,
            'size': len(content),
            'lines': len(content.split('\n')),
            'depth': depth,
            'imports': self._extract_imports(content, file_path),
            'exports': self._extract_exports(content),
            'functions': self._extract_functions(content),
            'classes': self._extract_classes(content),
            'dependencies': set(),
            'dependents': set(),
            'last_modified': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
            'status': FileStatus.ACTIVE,
        }
        
        # Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ñ–
        for import_path in file_info['imports']:
            dep_path = self._resolve_import(file_path, import_path)
            if dep_path:
                try:
                    dep_key = str(dep_path.relative_to(self.project_root))
                    file_info['dependencies'].add(dep_key)
                    self.dependencies[file_key].add(dep_key)
                    self.reverse_dependencies[dep_key].add(file_key)
                except ValueError:
                    pass
        
        self.files_cache[file_key] = file_info
        
        # Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·ÑƒÑ”Ğ¼Ğ¾ Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ñ–
        if depth < max_depth:
            for dep_key in file_info['dependencies']:
                dep_path = self.project_root / dep_key
                if dep_path.exists():
                    self._analyze_file(dep_path, depth + 1, max_depth)
    
    def _extract_imports(self, content: str, file_path: Path) -> List[str]:
        """Ğ’Ğ¸Ñ‚ÑĞ³Ñ‚Ğ¸ Ñ–Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸ Ğ· Ñ„Ğ°Ğ¹Ğ»Ñƒ"""
        imports = []
        
        if file_path.suffix in ['.py']:
            # Python Ñ–Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· AST
            try:
                tree = ast.parse(content)
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            imports.append(alias.name)
                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            imports.append(node.module)
            except SyntaxError as e:
                logger.warning(f"Syntax error in {file_path}: {e}")
                try:
                    file_key = str(file_path.relative_to(self.project_root))
                except ValueError:
                    file_key = str(file_path)
                self.broken_files[file_key] = str(e)
                return []
            except Exception:
                pass
        else:
            # JavaScript/TypeScript Ñ–Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸
            import_pattern = r"import\s+(?:.*?)\s+from\s+['\"]([^'\"]+)['\"]"
            imports.extend(re.findall(import_pattern, content))
            
            require_pattern = r"require\s*\(\s*['\"]([^'\"]+)['\"]\s*\)"
            imports.extend(re.findall(require_pattern, content))
        
        return list(set(imports))
    
    def _extract_exports(self, content: str) -> List[str]:
        """Ğ’Ğ¸Ñ‚ÑĞ³Ñ‚Ğ¸ ĞµĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸ Ğ· Ñ„Ğ°Ğ¹Ğ»Ñƒ"""
        exports = []
        
        export_pattern = r"export\s+(?:default\s+)?(?:class|function|const|let|var)\s+(\w+)"
        exports.extend(re.findall(export_pattern, content))
        
        named_export_pattern = r"export\s+{\s*([^}]+)\s*}"
        matches = re.findall(named_export_pattern, content)
        for match in matches:
            for e in match.split(','):
                parts = e.strip().split()
                if parts:
                    exports.append(parts[0])
        
        return list(set(exports))
    
    def _extract_functions(self, content: str) -> List[Dict[str, Any]]:
        """Ğ’Ğ¸Ñ‚ÑĞ³Ñ‚Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ— Ğ· Ñ„Ğ°Ğ¹Ğ»Ñƒ"""
        functions = []
        
        func_pattern = r"(?:async\s+)?(?:def|function|const|let)\s+(\w+)\s*(?:\(|=)"
        matches = re.finditer(func_pattern, content)
        
        for match in matches:
            func_name = match.group(1)
            line_num = content[:match.start()].count('\n') + 1
            functions.append({
                'name': func_name,
                'line': line_num,
                'type': 'function'
            })
        
        return functions
    
    def _extract_classes(self, content: str) -> List[Dict[str, Any]]:
        """Ğ’Ğ¸Ñ‚ÑĞ³Ñ‚Ğ¸ ĞºĞ»Ğ°ÑĞ¸ Ğ· Ñ„Ğ°Ğ¹Ğ»Ñƒ"""
        classes = []
        
        class_pattern = r"class\s+(\w+)(?:\s+extends\s+(\w+))?"
        matches = re.finditer(class_pattern, content)
        
        for match in matches:
            class_name = match.group(1)
            extends = match.group(2)
            line_num = content[:match.start()].count('\n') + 1
            classes.append({
                'name': class_name,
                'extends': extends,
                'line': line_num,
                'type': 'class'
            })
        
        return classes
    
    def _resolve_import(self, from_file: Path, import_path: str) -> Optional[Path]:
        """Ğ Ğ¾Ğ·Ğ²'ÑĞ·Ğ°Ñ‚Ğ¸ Ñ–Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ Ğ´Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ñƒ"""
        if import_path.startswith('.'):
            resolved = (from_file.parent / import_path).resolve()
            
            for ext in ['.js', '.ts', '.py', '/index.js', '/index.ts']:
                test_path = Path(str(resolved) + ext) if not resolved.suffix else resolved
                if test_path.exists():
                    return test_path
        else:
            for possible_path in self.project_root.rglob(f"{import_path}*"):
                if possible_path.is_file() and possible_path.suffix in ['.js', '.ts', '.py']:
                    return possible_path
        
        return None
    
    def _determine_file_status(self):
        """Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚Ğ¸ ÑÑ‚Ğ°Ñ‚ÑƒÑ Ñ„Ğ°Ğ¹Ğ»Ñƒ"""
        for file_key, file_info in self.files_cache.items():
            if file_key in self.broken_files:
                self.file_status[file_key] = FileStatus.BROKEN
                continue
            
            dependents = self.reverse_dependencies.get(file_key, set())
            dependencies = file_info['dependencies']
            last_modified = datetime.fromisoformat(file_info['last_modified'])
            
            # Ğ’Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ”Ğ¼Ğ¾ ÑÑ‚Ğ°Ñ‚ÑƒÑ
            if self._is_entry_point(file_key):
                status = FileStatus.ACTIVE
            elif self._is_config_file(file_key):
                status = FileStatus.ACTIVE
            elif not dependents and not dependencies:
                status = FileStatus.UNUSED
            elif self._is_deprecated(file_key, last_modified):
                status = FileStatus.DEPRECATED
            elif self._is_legacy(file_key):
                status = FileStatus.LEGACY
            elif self._is_in_development(file_key):
                status = FileStatus.IN_DEVELOPMENT
            else:
                status = FileStatus.ACTIVE
            
            self.file_status[file_key] = status
    
    def _is_entry_point(self, file_key: str) -> bool:
        """ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸, Ñ‡Ğ¸ Ñ„Ğ°Ğ¹Ğ» Ñ” Ñ‚Ğ¾Ñ‡ĞºĞ¾Ñ Ğ²Ñ…Ğ¾Ğ´Ñƒ"""
        entry_points = ['index.js', 'main.js', 'app.js', '__main__.py', 'main.py']
        return any(file_key.endswith(ep) for ep in entry_points)
    
    def _is_config_file(self, file_key: str) -> bool:
        """ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸, Ñ‡Ğ¸ Ñ„Ğ°Ğ¹Ğ» Ñ” ĞºĞ¾Ğ½Ñ„Ñ–Ğ³ÑƒÑ€Ğ°Ñ†Ñ–Ğ¹Ğ½Ğ¸Ğ¼"""
        config_patterns = ['config', 'settings', 'env', 'package.json', 'tsconfig']
        return any(pattern in file_key.lower() for pattern in config_patterns)
    
    def _is_deprecated(self, file_key: str, last_modified: datetime) -> bool:
        """ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸, Ñ‡Ğ¸ Ñ„Ğ°Ğ¹Ğ» Ğ·Ğ°ÑÑ‚Ğ°Ñ€Ñ–Ğ»Ğ¸Ğ¹"""
        days_since_modified = (datetime.now() - last_modified).days
        return days_since_modified > self.deprecated_threshold_days
    
    def _is_legacy(self, file_key: str) -> bool:
        """ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸, Ñ‡Ğ¸ Ñ„Ğ°Ğ¹Ğ» legacy"""
        legacy_patterns = ['legacy', 'old', 'deprecated', 'archive']
        return any(pattern in file_key.lower() for pattern in legacy_patterns)
    
    def _is_in_development(self, file_key: str) -> bool:
        """ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€Ğ¸Ñ‚Ğ¸, Ñ‡Ğ¸ Ñ„Ğ°Ğ¹Ğ» Ğ² Ñ€Ğ¾Ğ·Ñ€Ğ¾Ğ±Ñ†Ñ–"""
        dev_patterns = ['dev', 'wip', 'experimental', 'test']
        return any(pattern in file_key.lower() for pattern in dev_patterns)
    
    def _detect_circular_dependencies(self) -> List[List[str]]:
        """Ğ’Ğ¸ÑĞ²Ğ¸Ñ‚Ğ¸ Ñ†Ğ¸ĞºĞ»Ñ–Ñ‡Ğ½Ñ– Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ñ– Ñ‡ĞµÑ€ĞµĞ· DFS"""
        cycles = []
        visited = set()
        rec_stack = set()
        
        def dfs(node, path):
            visited.add(node)
            rec_stack.add(node)
            
            for neighbor in self.dependencies.get(node, []):
                if neighbor not in visited:
                    dfs(neighbor, path + [neighbor])
                elif neighbor in rec_stack:
                    cycle = path + [neighbor]
                    cycles.append(cycle)
            
            rec_stack.remove(node)
        
        for node in self.dependencies:
            if node not in visited:
                dfs(node, [node])
        
        return cycles
    
    def _build_architecture_map(self, max_depth: int, cycles: List) -> Dict[str, Any]:
        """ĞŸĞ¾Ğ±ÑƒĞ´ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñƒ ĞºĞ°Ñ€Ñ‚Ñƒ"""
        return {
            'timestamp': datetime.now().isoformat(),
            'project_root': str(self.project_root),
            'analysis_root': str(self.analysis_root),
            'max_depth': max_depth,
            'files': self._serialize_files(),
            'dependencies': self._serialize_dependencies(),
            'circular_dependencies': cycles,
            'statistics': self._calculate_statistics(),
            'health_score': self._calculate_health_score(),
        }
    
    def _serialize_files(self) -> Dict[str, Any]:
        """Ğ¡ĞµÑ€Ñ–Ğ°Ğ»Ñ–Ğ·ÑƒĞ²Ğ°Ñ‚Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ¸ Ğ´Ğ»Ñ JSON"""
        result = {}
        for file_key, file_info in self.files_cache.items():
            result[file_key] = {
                'size': file_info['size'],
                'lines': file_info['lines'],
                'depth': file_info['depth'],
                'status': self.file_status.get(file_key, FileStatus.ACTIVE),
                'imports_count': len(file_info['imports']),
                'exports_count': len(file_info['exports']),
                'functions_count': len(file_info['functions']),
                'classes_count': len(file_info['classes']),
                'dependencies_count': len(file_info['dependencies']),
                'dependents_count': len(self.reverse_dependencies.get(file_key, [])),
                'last_modified': file_info['last_modified'],
                'functions': file_info['functions'],
                'classes': file_info['classes'],
                'broken_reason': self.broken_files.get(file_key),
            }
        return result
    
    def _serialize_dependencies(self) -> Dict[str, List[str]]:
        """Ğ¡ĞµÑ€Ñ–Ğ°Ğ»Ñ–Ğ·ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ñ–"""
        result = {}
        for file_key, deps in self.dependencies.items():
            result[file_key] = sorted(list(deps))
        return result
    
    def _calculate_statistics(self) -> Dict[str, Any]:
        """Ğ Ğ¾Ğ·Ñ€Ğ°Ñ…ÑƒĞ²Ğ°Ñ‚Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ"""
        total_files = len(self.files_cache)
        total_lines = sum(f['lines'] for f in self.files_cache.values())
        total_size = sum(f['size'] for f in self.files_cache.values())
        
        status_counts = defaultdict(int)
        for status in self.file_status.values():
            status_counts[status] += 1
        
        return {
            'total_files': total_files,
            'total_lines': total_lines,
            'total_size': total_size,
            'average_lines_per_file': total_lines // total_files if total_files > 0 else 0,
            'status_distribution': dict(status_counts),
            'unused_files': sum(1 for s in self.file_status.values() if s == FileStatus.UNUSED),
            'active_files': sum(1 for s in self.file_status.values() if s == FileStatus.ACTIVE),
            'deprecated_files': sum(1 for s in self.file_status.values() if s == FileStatus.DEPRECATED),
            'broken_files': sum(1 for s in self.file_status.values() if s == FileStatus.BROKEN),
        }
    
    def _calculate_health_score(self) -> Dict[str, Any]:
        """Ğ Ğ¾Ğ·Ñ€Ğ°Ñ…ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ¾Ñ†Ñ–Ğ½ĞºÑƒ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²'Ñ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¸"""
        stats = self._calculate_statistics()
        
        unused_ratio = stats['unused_files'] / stats['total_files'] if stats['total_files'] > 0 else 0
        avg_deps = sum(len(d) for d in self.dependencies.values()) / len(self.dependencies) if self.dependencies else 0
        
        score = 100
        score -= unused_ratio * 20
        
        if avg_deps > 5:
            score -= (avg_deps - 5) * 2
        
        if stats['total_files'] > 10:
            score += 10
        
        return {
            'score': max(0, min(100, score)),
            'unused_ratio': unused_ratio,
            'average_dependencies': avg_deps,
            'modularity': 'good' if stats['total_files'] > 10 else 'fair',
        }
    
    def export_architecture(self, output_path: Path) -> None:
        """Ğ•ĞºÑĞ¿Ğ¾Ñ€Ñ‚ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ² JSON"""
        architecture = self.analyze_architecture()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(architecture, f, indent=2, default=str)
        
        logger.info(f"âœ… ĞÑ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ĞµĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ² {output_path}")
    
    def generate_architecture_report(self) -> str:
        """Ğ“ĞµĞ½ĞµÑ€ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ·Ğ²Ñ–Ñ‚ Ğ¿Ñ€Ğ¾ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ"""
        architecture = self.analyze_architecture()
        stats = architecture['statistics']
        health = architecture['health_score']
        
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           ĞĞ Ğ¥Ğ†Ğ¢Ğ•ĞšĞ¢Ğ£Ğ ĞĞ ĞšĞĞ Ğ¢Ğ Ğ¡Ğ˜Ğ¡Ğ¢Ğ•ĞœĞ˜                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ:
   â€¢ Ğ’ÑÑŒĞ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ²: {stats['total_files']}
   â€¢ ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸Ñ… Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ²: {stats['active_files']}
   â€¢ ĞĞµĞ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ²Ğ°Ğ½Ğ¸Ñ… Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ²: {stats['unused_files']}
   â€¢ Ğ—Ğ°ÑÑ‚Ğ°Ñ€Ñ–Ğ»Ğ¸Ñ… Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ²: {stats['deprecated_files']}
   â€¢ Ğ’ÑÑŒĞ¾Ğ³Ğ¾ Ñ€ÑĞ´ĞºÑ–Ğ² ĞºĞ¾Ğ´Ñƒ: {stats['total_lines']}
   â€¢ Ğ¡ĞµÑ€ĞµĞ´Ğ½Ñ Ğ´Ğ¾Ğ²Ğ¶Ğ¸Ğ½Ğ° Ñ„Ğ°Ğ¹Ğ»Ñƒ: {stats['average_lines_per_file']} Ñ€ÑĞ´ĞºÑ–Ğ²
   â€¢ Ğ—Ğ°Ğ³Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¹ Ñ€Ğ¾Ğ·Ğ¼Ñ–Ñ€: {stats['total_size']} Ğ±Ğ°Ğ¹Ñ‚

ğŸ¥ Ğ—Ğ”ĞĞ ĞĞ’'Ğ¯ ĞĞ Ğ¥Ğ†Ğ¢Ğ•ĞšĞ¢Ğ£Ğ Ğ˜:
   â€¢ ĞÑ†Ñ–Ğ½ĞºĞ°: {health['score']:.1f}/100
   â€¢ ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ–ÑÑ‚ÑŒ: {health['modularity']}
   â€¢ ĞšĞ¾ĞµÑ„Ñ–Ñ†Ñ–Ñ”Ğ½Ñ‚ Ğ½ĞµĞ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ°Ğ½Ğ½Ñ: {health['unused_ratio']:.1%}
   â€¢ Ğ¡ĞµÑ€ĞµĞ´Ğ½Ñ Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ñ–ÑÑ‚ÑŒ: {health['average_dependencies']:.1f}

ğŸ“ Ğ ĞĞ—ĞŸĞĞ”Ğ†Ğ› Ğ¡Ğ¢ĞĞ¢Ğ£Ğ¡Ğ†Ğ’:
"""
        for status, count in stats['status_distribution'].items():
            report += f"   â€¢ {status}: {count}\n"
        
        return report


if __name__ == '__main__':
    project_root = Path(__file__).parent.parent.parent
    mapper = ArchitectureMapper(project_root)
    
    print(mapper.generate_architecture_report())
    
    output_path = Path(__file__).parent.parent / 'reports' / 'architecture_map.json'
    output_path.parent.mkdir(parents=True, exist_ok=True)
    mapper.export_architecture(output_path)
