#!/usr/bin/env python3
"""
MCP Architecture Server v2.0 - –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∑ Windsurf IDE
–ó–∞–±–µ–∑–ø–µ—á—É—î MCP —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ —Ç–∞ WebSocket –¥–ª—è real-time –æ–Ω–æ–≤–ª–µ–Ω—å
"""

import json
import sys
import os
import asyncio
import time
from pathlib import Path
from typing import Any, Dict, List, Optional
from datetime import datetime
import logging
from dotenv import load_dotenv

# –î–æ–¥–∞—î–º–æ codemap-system –¥–æ PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.architecture_mapper import ArchitectureMapper, FileStatus
from core.code_duplication_detector import CodeDuplicationDetector
from core.code_quality_analyzer import CodeQualityAnalyzer

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/mcp_architecture_server.log'),
        logging.StreamHandler(sys.stderr)
    ]
)
logger = logging.getLogger(__name__)


class ArchitectureAnalysisServer:
    """MCP —Å–µ—Ä–≤–µ—Ä –¥–ª—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –∑ Windsurf —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—î—é"""
    
    def __init__(self):
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ .env.architecture –∑ –ø–∞–ø–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞
        env_path = Path(__file__).parent.parent / '.env.architecture'
        if env_path.exists():
            load_dotenv(env_path)
        
        # –ß–∏—Ç–∞—î–º–æ PROJECT_ROOT –∑ .env.architecture
        project_root = os.environ.get('PROJECT_ROOT', '..')
        
        # –Ø–∫—â–æ –≤—ñ–¥–Ω–æ—Å–Ω–∏–π —à–ª—è—Ö, —Ä–æ–±–∏–º–æ –π–æ–≥–æ –∞–±—Å–æ–ª—é—Ç–Ω–∏–º –≤—ñ–¥ codemap-system
        if not os.path.isabs(project_root):
            project_root = Path(__file__).parent.parent / project_root
        
        self.project_root = Path(project_root).resolve()
        self.codemap_dir = Path(__file__).parent.parent
        self.reports_dir = self.codemap_dir / "reports"
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏
        self.mapper = ArchitectureMapper(self.project_root)
        self.duplication_detector = CodeDuplicationDetector(self.project_root)
        self.quality_analyzer = CodeQualityAnalyzer(self.project_root)
        
        # –ö–µ—à –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏
        self.architecture_cache = None
        self.last_analysis_time = None
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
        self._setup_logging()
        
        self.tools = self._initialize_architecture_tools()
        
        logger.info("üöÄ MCP Architecture Server —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π")
    
    def _setup_logging(self):
        """–ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ –ª–æ–≥—É–≤–∞–Ω–Ω—è"""
        logs_dir = self.codemap_dir / "logs"
        logs_dir.mkdir(parents=True, exist_ok=True)
        
        log_file = logs_dir / "mcp_architecture_server.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stderr)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _initialize_architecture_tools(self) -> List[Dict[str, Any]]:
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏"""
        return [
            {
                "name": "get_architecture_overview",
                "description": "–û—Ç—Ä–∏–º–∞—Ç–∏ –æ–≥–ª—è–¥ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏: –∞–∫—Ç–∏–≤–Ω—ñ —Ñ–∞–π–ª–∏, –Ω–µ–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–Ω—ñ, –∑–¥–æ—Ä–æ–≤'—è —Å–∏—Å—Ç–µ–º–∏",
                "inputSchema": {
                    "type": "object",
                    "properties": {}
                }
            },
            {
                "name": "analyze_file_status",
                "description": "–ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Å—Ç–∞—Ç—É—Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª—É: –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ, –∑–∞–ª–µ–∂–Ω—ñ —Ñ–∞–π–ª–∏, –∑–¥–æ—Ä–æ–≤'—è",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string", "description": "–®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É"}
                    },
                    "required": ["file_path"]
                }
            },
            {
                "name": "detect_circular_dependencies",
                "description": "–í–∏—è–≤–∏—Ç–∏ —Ü–∏–∫–ª—ñ—á–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—ñ",
                "inputSchema": {
                    "type": "object",
                    "properties": {}
                }
            },
            {
                "name": "detect_unused_files",
                "description": "–í–∏—è–≤–∏—Ç–∏ –Ω–µ–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–Ω—ñ —Ñ–∞–π–ª–∏",
                "inputSchema": {
                    "type": "object",
                    "properties": {}
                }
            },
            {
                "name": "detect_duplicates",
                "description": "–í–∏—è–≤–∏—Ç–∏ –¥—É–±–ª—ñ–∫–∞—Ç–∏ –∫–æ–¥—É",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "min_lines": {"type": "integer", "description": "–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –±–ª–æ–∫—É (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º 5)"}
                    }
                }
            },
            {
                "name": "detect_broken_files",
                "description": "–í–∏—è–≤–∏—Ç–∏ —Ñ–∞–π–ª–∏ –∑ –ø–æ–º–∏–ª–∫–∞–º–∏ (syntax error)",
                "inputSchema": {
                    "type": "object",
                    "properties": {}
                }
            },
            {
                "name": "get_dependency_graph",
                "description": "–û—Ç—Ä–∏–º–∞—Ç–∏ –≥—Ä–∞—Ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è —Ñ–∞–π–ª—É",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string", "description": "–®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É"},
                        "depth": {"type": "integer", "description": "–ì–ª–∏–±–∏–Ω–∞ –≥—Ä–∞—Ñ—É"}
                    },
                    "required": ["file_path"]
                }
            },
            {
                "name": "get_architecture_health",
                "description": "–û—Ç—Ä–∏–º–∞—Ç–∏ –æ—Ü—ñ–Ω–∫—É –∑–¥–æ—Ä–æ–≤'—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ —Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó",
                "inputSchema": {
                    "type": "object",
                    "properties": {}
                }
            },
            {
                "name": "get_refactoring_recommendations",
                "description": "–û—Ç—Ä–∏–º–∞—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∞–Ω–∞–ª—ñ–∑—É",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "priority": {"type": "string", "enum": ["low", "medium", "high"]}
                    }
                }
            },
            {
                "name": "export_architecture_report",
                "description": "–ï–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "format": {"type": "string", "enum": ["json", "html", "markdown"]}
                    }
                }
            }
        ]
    
    def _ensure_architecture_cached(self):
        """–ü–µ—Ä–µ–∫–æ–Ω–∞—Ç–∏—Å—è, —â–æ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–∞"""
        if self.architecture_cache is None:
            logger.info("üîç –ê–Ω–∞–ª—ñ–∑ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏...")
            self.architecture_cache = self.mapper.analyze_architecture(max_depth=2)
            self.last_analysis_time = datetime.now()
            logger.info("‚úÖ –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–∞")
    
    def handle_tool_call(self, tool_name: str, arguments: Dict[str, Any]) -> str:
        """–û–±—Ä–æ–±–∏—Ç–∏ –≤–∏–∫–ª–∏–∫ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—É"""
        try:
            if tool_name == "get_architecture_overview":
                return self._get_architecture_overview()
            elif tool_name == "analyze_file_status":
                file_path = arguments.get("file_path", "")
                return self._analyze_file_status(file_path)
            elif tool_name == "detect_circular_dependencies":
                return self._detect_circular_dependencies()
            elif tool_name == "detect_unused_files":
                return self._detect_unused_files()
            elif tool_name == "detect_duplicates":
                min_lines = arguments.get("min_lines", 5)
                return self._detect_duplicates(min_lines)
            elif tool_name == "detect_broken_files":
                return self._detect_broken_files()
            elif tool_name == "get_dependency_graph":
                file_path = arguments.get("file_path", "")
                depth = arguments.get("depth", 2)
                return self._get_dependency_graph(file_path, depth)
            elif tool_name == "get_architecture_health":
                return self._get_architecture_health()
            elif tool_name == "get_refactoring_recommendations":
                priority = arguments.get("priority", "medium")
                return self._get_refactoring_recommendations(priority)
            elif tool_name == "export_architecture_report":
                format_type = arguments.get("format", "json")
                return self._export_architecture_report(format_type)
            else:
                return json.dumps({"error": f"Unknown tool: {tool_name}"})
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤ {tool_name}: {e}", exc_info=True)
            return json.dumps({"error": str(e)})
    
    def _get_architecture_overview(self) -> str:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –æ–≥–ª—è–¥ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏"""
        self._ensure_architecture_cached()
        
        if self.architecture_cache is None:
            return json.dumps({"error": "Architecture not analyzed yet"})
        
        stats = self.architecture_cache.get("statistics", {})
        health = self.architecture_cache.get("health_score", {})
        cycles = self.architecture_cache.get("circular_dependencies", [])
        
        overview = {
            "timestamp": self.last_analysis_time.isoformat() if self.last_analysis_time else None,
            "statistics": stats,
            "health_score": health,
            "circular_dependencies_count": len(cycles),
            "summary": {
                "total_files": stats.get("total_files", 0),
                "active_files": stats.get("active_files", 0),
                "unused_files": stats.get("unused_files", 0),
                "deprecated_files": stats.get("deprecated_files", 0),
                "broken_files": stats.get("broken_files", 0),
            }
        }
        
        return json.dumps(overview, indent=2, ensure_ascii=False)
    
    def _analyze_file_status(self, file_path: str) -> str:
        """–ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Å—Ç–∞—Ç—É—Å —Ñ–∞–π–ª—É"""
        self._ensure_architecture_cached()
        
        if self.architecture_cache is None:
            return json.dumps({"error": "Architecture not analyzed yet"})
        
        if file_path not in self.architecture_cache.get("files", {}):
            return json.dumps({"error": f"File not found: {file_path}"})
        
        file_info = self.architecture_cache["files"][file_path]
        
        status_info = {
            "file": file_path,
            "status": file_info.get("status"),
            "size": file_info.get("size"),
            "lines": file_info.get("lines"),
            "last_modified": file_info.get("last_modified"),
            "functions_count": file_info.get("functions_count"),
            "classes_count": file_info.get("classes_count"),
            "dependencies": self.architecture_cache.get("dependencies", {}).get(file_path, []),
            "dependents_count": file_info.get("dependents_count"),
        }
        
        return json.dumps(status_info, indent=2, ensure_ascii=False)
    
    def _detect_circular_dependencies(self) -> str:
        """–í–∏—è–≤–∏—Ç–∏ —Ü–∏–∫–ª—ñ—á–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ"""
        self._ensure_architecture_cached()
        
        if self.architecture_cache is None:
            return json.dumps({"error": "Architecture not analyzed yet"})
        
        cycles = self.architecture_cache.get("circular_dependencies", [])
        
        return json.dumps({
            "circular_dependencies": cycles,
            "count": len(cycles),
            "message": f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(cycles)} —Ü–∏–∫–ª—ñ—á–Ω–∏—Ö –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π"
        }, indent=2, ensure_ascii=False)
    
    def _detect_unused_files(self) -> str:
        """–í–∏—è–≤–∏—Ç–∏ –Ω–µ–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–Ω—ñ —Ñ–∞–π–ª–∏"""
        self._ensure_architecture_cached()
        
        if self.architecture_cache is None:
            return json.dumps({"error": "Architecture not analyzed yet"})
        
        unused = []
        for file_path, file_info in self.architecture_cache.get("files", {}).items():
            if file_info.get("status") == FileStatus.UNUSED:
                unused.append({
                    "file": file_path,
                    "size": file_info.get("size"),
                    "lines": file_info.get("lines")
                })
        
        return json.dumps({
            "unused_files": unused,
            "count": len(unused),
            "message": f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(unused)} –Ω–µ–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤"
        }, indent=2, ensure_ascii=False)
    
    def _detect_duplicates(self, min_lines: int) -> str:
        """–í–∏—è–≤–∏—Ç–∏ –¥—É–±–ª—ñ–∫–∞—Ç–∏"""
        logger.info(f"üîç –ü–æ—à—É–∫ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ (min_lines={min_lines})...")
        
        try:
            duplicates = self.duplication_detector.find_duplicates(min_lines=min_lines)
            
            return json.dumps({
                "duplicates": duplicates[:10],  # –ü–æ–∫–∞–∑—É—î–º–æ –ø–µ—Ä—à—ñ 10
                "count": len(duplicates),
                "message": f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(duplicates)} –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ –∫–æ–¥—É"
            }, indent=2, ensure_ascii=False, default=str)
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–æ—à—É–∫—É –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {e}")
            return json.dumps({"error": str(e)})
    
    def _detect_broken_files(self) -> str:
        """–í–∏—è–≤–∏—Ç–∏ —Ñ–∞–π–ª–∏ –∑ –ø–æ–º–∏–ª–∫–∞–º–∏"""
        self._ensure_architecture_cached()
        
        if self.architecture_cache is None:
            return json.dumps({"error": "Architecture not analyzed yet"})
        
        broken = []
        for file_path, file_info in self.architecture_cache.get("files", {}).items():
            if file_info.get("status") == FileStatus.BROKEN or file_info.get("broken_reason"):
                broken.append({
                    "file": file_path,
                    "size": file_info.get("size"),
                    "lines": file_info.get("lines"),
                    "reason": file_info.get("broken_reason"),
                })
        
        return json.dumps({
            "broken_files": broken,
            "count": len(broken),
            "message": f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(broken)} —Ñ–∞–π–ª—ñ–≤ –∑ –ø–æ–º–∏–ª–∫–∞–º–∏",
        }, indent=2, ensure_ascii=False)
    
    def _get_dependency_graph(self, file_path: str, depth: int) -> str:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –≥—Ä–∞—Ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π"""
        self._ensure_architecture_cached()
        
        if self.architecture_cache is None:
            return json.dumps({"error": "Architecture not analyzed yet"})
        
        if file_path not in self.architecture_cache.get("files", {}):
            return json.dumps({"error": f"File not found: {file_path}"})
        
        graph = {
            "root": file_path,
            "depth": depth,
            "nodes": {},
            "edges": []
        }
        
        # –ë—É–¥—É—î–º–æ –≥—Ä–∞—Ñ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
        self._build_graph_recursive(file_path, graph, depth, 0)
        
        return json.dumps(graph, indent=2, ensure_ascii=False)
    
    def _build_graph_recursive(self, file_path: str, graph: Dict, max_depth: int, current_depth: int):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –±—É–¥—É–≤–∞—Ç–∏ –≥—Ä–∞—Ñ"""
        if current_depth >= max_depth or file_path in graph["nodes"]:
            return
        
        if self.architecture_cache is None:
            return
        
        if file_path not in self.architecture_cache.get("files", {}):
            return
        
        file_info = self.architecture_cache["files"][file_path]
        graph["nodes"][file_path] = {
            "status": file_info.get("status"),
            "lines": file_info.get("lines"),
            "depth": current_depth
        }
        
        deps = self.architecture_cache.get("dependencies", {}).get(file_path, [])
        for dep in deps:
            graph["edges"].append({"from": file_path, "to": dep})
            self._build_graph_recursive(dep, graph, max_depth, current_depth + 1)
    
    def _get_architecture_health(self) -> str:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –æ—Ü—ñ–Ω–∫—É –∑–¥–æ—Ä–æ–≤'—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏"""
        self._ensure_architecture_cached()
        
        if self.architecture_cache is None:
            return json.dumps({"error": "Architecture not analyzed yet"})
        
        health = self.architecture_cache.get("health_score", {})
        
        return json.dumps({
            "health": health,
            "recommendations": self._generate_health_recommendations(health)
        }, indent=2, ensure_ascii=False)
    
    def _generate_health_recommendations(self, health: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∑–¥–æ—Ä–æ–≤'—è"""
        recommendations = []
        score = health.get("score", 0)
        
        if score < 50:
            recommendations.append("‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ù–û: –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ—Ç—Ä–µ–±—É—î —Å–µ—Ä–π–æ–∑–Ω–æ–≥–æ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥—É")
        elif score < 70:
            recommendations.append("‚ö†Ô∏è –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –º–∞—î –ø—Ä–æ–±–ª–µ–º–∏, —Ä–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è")
        else:
            recommendations.append("‚úÖ –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –≤ —Ö–æ—Ä–æ—à–æ–º—É —Å—Ç–∞–Ω—ñ")
        
        return recommendations
    
    def _get_refactoring_recommendations(self, priority: str) -> str:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥—É"""
        self._ensure_architecture_cached()
        
        if self.architecture_cache is None:
            return json.dumps({"error": "Architecture not analyzed yet"})
        
        recommendations = {
            "priority": priority,
            "items": []
        }
        
        # –ê–Ω–∞–ª—ñ–∑—É—î–º–æ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É —Ç–∞ –≥–µ–Ω–µ—Ä—É—î–º–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
        for file_path, file_info in self.architecture_cache.get("files", {}).items():
            if file_info.get("status") == FileStatus.DEPRECATED:
                recommendations["items"].append({
                    "file": file_path,
                    "type": "deprecated",
                    "action": "–†–æ–∑–≥–ª—è–Ω—É—Ç–∏ –≤–∏–¥–∞–ª–µ–Ω–Ω—è –∞–±–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è"
                })
            elif file_info.get("status") == FileStatus.UNUSED:
                recommendations["items"].append({
                    "file": file_path,
                    "type": "unused",
                    "action": "–†–æ–∑–≥–ª—è–Ω—É—Ç–∏ –≤–∏–¥–∞–ª–µ–Ω–Ω—è"
                })
        
        return json.dumps(recommendations, indent=2, ensure_ascii=False)
    
    def _export_architecture_report(self, format_type: str) -> str:
        """–ï–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–≤—ñ—Ç –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏"""
        self._ensure_architecture_cached()
        
        report_path = self.reports_dir / f"architecture_report.{format_type}"
        
        if format_type == "json":
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(self.architecture_cache, f, indent=2, ensure_ascii=False, default=str)
        elif format_type == "markdown":
            md_content = self._generate_markdown_report()
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(md_content)
        
        return json.dumps({
            "success": True,
            "report_path": str(report_path),
            "format": format_type,
            "message": f"–ó–≤—ñ—Ç –µ–∫—Å–ø–æ—Ä—Ç–æ–≤–∞–Ω–∏–π –≤ {report_path}"
        }, indent=2, ensure_ascii=False)
    
    def _generate_markdown_report(self) -> str:
        """–ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ Markdown –∑–≤—ñ—Ç"""
        if self.architecture_cache is None:
            return "# Architecture not analyzed"
        
        stats = self.architecture_cache.get("statistics", {})
        health = self.architecture_cache.get("health_score", {})
        cycles = self.architecture_cache.get("circular_dependencies", [])
        
        md = f"""# üèóÔ∏è Architecture Report

## üìä Statistics
- **Total Files**: {stats.get('total_files', 0)}
- **Active Files**: {stats.get('active_files', 0)}
- **Unused Files**: {stats.get('unused_files', 0)}
- **Deprecated Files**: {stats.get('deprecated_files', 0)}
- **Total Lines**: {stats.get('total_lines', 0)}

## üè• Health
- **Score**: {health.get('score', 0):.1f}/100
- **Modularity**: {health.get('modularity', 'unknown')}
- **Unused Ratio**: {health.get('unused_ratio', 0):.1%}

## üîÑ Circular Dependencies
- **Count**: {len(cycles)}
"""
        
        if cycles:
            md += "\n### Cycles Found:\n"
            for i, cycle in enumerate(cycles[:5], 1):
                md += f"{i}. {' ‚Üí '.join(cycle)}\n"
        
        return md


def main():
    """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è MCP —Å–µ—Ä–≤–µ—Ä–∞"""
    server = ArchitectureAnalysisServer()
    
    # –Ø–∫—â–æ stdin –∑–∞–∫—Ä–∏—Ç–∏–π –∞–±–æ –Ω–µ TTY (–∑–∞–ø—É—â–µ–Ω–æ –≤ —Ñ–æ–Ω—ñ), –ø—Ä–æ—Å—Ç–æ —á–µ–∫–∞—î–º–æ
    try:
        is_tty = sys.stdin.isatty()
    except (AttributeError, ValueError):
        is_tty = False
    
    if not is_tty:
        logger.info("üì° MCP —Å–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤–∏–π –¥–æ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–º–∞–Ω–¥ (—Ñ–æ–Ω–æ–≤–∏–π —Ä–µ–∂–∏–º)")
        # –ß–µ–∫–∞—î–º–æ –Ω–∞ —Å–∏–≥–Ω–∞–ª–∏, –∞–ª–µ –Ω–µ —á–∏—Ç–∞—î–º–æ stdin
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            logger.info("üõë MCP —Å–µ—Ä–≤–µ—Ä –∑—É–ø–∏–Ω–µ–Ω–∏–π")
            return
    
    # –ß–∏—Ç–∞—î–º–æ JSON-RPC –∑–∞–ø–∏—Ç–∏ –∑—ñ stdin
    for line in sys.stdin:
        try:
            request = json.loads(line)
            
            if request.get("method") == "initialize":
                response = {
                    "jsonrpc": "2.0",
                    "id": request.get("id"),
                    "result": {
                        "protocolVersion": "2024-11-05",
                        "capabilities": {},
                        "serverInfo": {
                            "name": "architecture-analysis-server",
                            "version": "2.0.0"
                        }
                    }
                }
            elif request.get("method") == "tools/list":
                response = {
                    "jsonrpc": "2.0",
                    "id": request.get("id"),
                    "result": {
                        "tools": server.tools
                    }
                }
            elif request.get("method") == "tools/call":
                params = request.get("params", {})
                tool_name = params.get("name")
                arguments = params.get("arguments", {})
                
                result = server.handle_tool_call(tool_name, arguments)
                
                response = {
                    "jsonrpc": "2.0",
                    "id": request.get("id"),
                    "result": {
                        "content": [
                            {
                                "type": "text",
                                "text": result
                            }
                        ]
                    }
                }
            else:
                response = {
                    "jsonrpc": "2.0",
                    "id": request.get("id"),
                    "error": {
                        "code": -32601,
                        "message": "Method not found"
                    }
                }
            
            print(json.dumps(response, ensure_ascii=False))
            sys.stdout.flush()
        
        except json.JSONDecodeError as e:
            error_response = {
                "jsonrpc": "2.0",
                "error": {
                    "code": -32700,
                    "message": f"Parse error: {e}"
                }
            }
            print(json.dumps(error_response))
            sys.stdout.flush()
        except Exception as e:
            error_response = {
                "jsonrpc": "2.0",
                "error": {
                    "code": -32603,
                    "message": f"Internal error: {e}"
                }
            }
            print(json.dumps(error_response))
            sys.stdout.flush()


if __name__ == "__main__":
    main()
