#!/usr/bin/env python3
"""
Architecture Mapper - Ğ“Ğ»Ğ¸Ğ±Ğ¾ĞºĞ° Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¸
Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ· Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ², Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹, ÑÑ‚Ğ°Ñ‚ÑƒÑÑ–Ğ² Ñ‚Ğ° Ğ²Ğ·Ğ°Ñ”Ğ¼Ğ¾Ğ´Ñ–Ğ¹
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any
from datetime import datetime
from collections import defaultdict
import ast
import re
from dotenv import load_dotenv


class FileStatus:
    """Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ñ„Ğ°Ğ¹Ğ»Ñƒ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ñ–"""
    ACTIVE = "ACTIVE"  # ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ”Ñ‚ÑŒÑÑ
    DEPRECATED = "DEPRECATED"  # Ğ—Ğ°ÑÑ‚Ğ°Ñ€Ñ–Ğ»Ğ¾
    UNUSED = "UNUSED"  # ĞĞµ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ”Ñ‚ÑŒÑÑ
    IN_DEVELOPMENT = "IN_DEVELOPMENT"  # Ğ’ Ñ€Ğ¾Ğ·Ñ€Ğ¾Ğ±Ñ†Ñ–
    LEGACY = "LEGACY"  # Ğ¡Ğ¿Ğ°Ğ´Ñ‰Ğ¸Ğ½Ğ°
    EXPERIMENTAL = "EXPERIMENTAL"  # Ğ•ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğµ


class ArchitectureMapper:
    """Ğ“Ğ»Ğ¸Ğ±Ğ¾ĞºĞ° Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¸ Ğ· Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¸Ğ¼ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ğ¾Ğ¼"""
    
    def __init__(self, project_root: Optional[Path] = None):
        # Ğ¯ĞºÑ‰Ğ¾ project_root Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½Ğ¾, Ñ‡Ğ¸Ñ‚Ğ°Ñ”Ğ¼Ğ¾ Ğ· .env.architecture
        if project_root is None:
            env_path = Path(__file__).parent / '.env.architecture'
            if env_path.exists():
                load_dotenv(env_path)
            
            project_root_str = os.environ.get('PROJECT_ROOT', '..')
            if not os.path.isabs(project_root_str):
                project_root = Path(__file__).parent / project_root_str
            else:
                project_root = Path(project_root_str)
        
        self.project_root = Path(project_root).resolve()
        
        # ĞĞ½Ğ°Ğ»Ñ–Ğ·ÑƒÑ”Ğ¼Ğ¾ Ğ²ĞµÑÑŒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚, Ğ° Ğ½Ğµ Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ workflow
        self.analysis_root = self.project_root
        
        # ĞšĞµÑˆ Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ² Ñ‚Ğ° Ñ—Ñ… Ğ²Ğ»Ğ°ÑÑ‚Ğ¸Ğ²Ğ¾ÑÑ‚ĞµĞ¹
        self.files_cache: Dict[str, Dict[str, Any]] = {}
        self.dependencies: Dict[str, Set[str]] = defaultdict(set)
        self.reverse_dependencies: Dict[str, Set[str]] = defaultdict(set)
        self.file_interactions: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        self.file_status: Dict[str, str] = {}
        self.last_modified: Dict[str, datetime] = {}
        
    def analyze_architecture(self, max_depth: int = 5) -> Dict[str, Any]:
        """ĞĞ½Ğ°Ğ»Ñ–Ğ·ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¸ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾"""
        print(f"ğŸ” ĞĞ½Ğ°Ğ»Ñ–Ğ· Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¸ Ğ½Ğ° Ğ³Ğ»Ğ¸Ğ±Ğ¸Ğ½Ñƒ {max_depth}...")
        
        # Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ²ÑÑ– Ñ„Ğ°Ğ¹Ğ»Ğ¸ workflow
        workflow_files = self._find_workflow_files()
        print(f"   ğŸ“ Ğ—Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(workflow_files)} Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ²")
        
        # ĞĞ½Ğ°Ğ»Ñ–Ğ·ÑƒÑ”Ğ¼Ğ¾ ĞºĞ¾Ğ¶ĞµĞ½ Ñ„Ğ°Ğ¹Ğ»
        for file_path in workflow_files:
            self._analyze_file(file_path, depth=0, max_depth=max_depth)
        
        # Ğ’Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ”Ğ¼Ğ¾ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ¸ Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ²
        self._determine_file_status()
        
        # Ğ‘ÑƒĞ´ÑƒÑ”Ğ¼Ğ¾ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñƒ ĞºĞ°Ñ€Ñ‚Ñƒ
        architecture = self._build_architecture_map(max_depth)
        
        return architecture
    
    def _find_workflow_files(self) -> List[Path]:
        """Ğ—Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ²ÑÑ– Ñ„Ğ°Ğ¹Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñƒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ"""
        files = []
        
        # Ğ Ğ¾Ğ·ÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ
        extensions = {'.js', '.ts', '.jsx', '.tsx', '.py', '.java', '.cpp', '.go'}
        
        # ĞŸĞ°Ğ¿ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ğ¸ĞºĞ»ÑÑ‡ĞµĞ½Ğ½Ñ
        exclude_dirs = {'node_modules', '__pycache__', '.git', '.venv', 'dist', 'build', 
                       'archive', '.archive', 'backups', '.cache', '.idx', '.vscode', '.DS_Store'}
        
        # Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ¿Ğ¾ Ğ²ÑÑ–Ğ¼ Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼
        for file_path in self.analysis_root.rglob('*'):
            # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ”Ğ¼Ğ¾ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ñ–Ñ—
            if file_path.is_dir():
                continue
            
            # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ”Ğ¼Ğ¾ Ğ¿Ñ€Ğ¸Ñ…Ğ¾Ğ²Ğ°Ğ½Ñ– Ñ„Ğ°Ğ¹Ğ»Ğ¸/Ğ¿Ğ°Ğ¿ĞºĞ¸
            if any(part.startswith('.') for part in file_path.parts):
                continue
            
            # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ”Ğ¼Ğ¾ Ğ²Ğ¸ĞºĞ»ÑÑ‡ĞµĞ½Ñ– Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ñ–Ñ—
            if any(part in exclude_dirs for part in file_path.parts):
                continue
            
            # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ÑÑ”Ğ¼Ğ¾ Ñ€Ğ¾Ğ·ÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ
            if file_path.suffix in extensions:
                files.append(file_path)
        
        return sorted(files)
    
    def _analyze_file(self, file_path: Path, depth: int = 0, max_depth: int = 5) -> Dict[str, Any]:
        """ĞĞ½Ğ°Ğ»Ñ–Ğ·ÑƒĞ²Ğ°Ñ‚Ğ¸ Ñ„Ğ°Ğ¹Ğ» Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾"""
        if depth > max_depth:
            return {}
        
        file_key = str(file_path.relative_to(self.project_root))
        
        # Ğ¯ĞºÑ‰Ğ¾ Ğ²Ğ¶Ğµ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·ÑƒĞ²Ğ°Ğ»Ğ¸, Ğ¿Ğ¾Ğ²ĞµÑ€Ñ‚Ğ°Ñ”Ğ¼Ğ¾ ĞºĞµÑˆ
        if file_key in self.files_cache:
            return self.files_cache[file_key]
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"   âš ï¸ ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ñ‡Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ {file_path}: {e}")
            return {}
        
        # ĞĞ½Ğ°Ğ»Ñ–Ğ·ÑƒÑ”Ğ¼Ğ¾ Ğ²Ğ¼Ñ–ÑÑ‚ Ñ„Ğ°Ğ¹Ğ»Ñƒ
        file_info = {
            'path': file_key,
            'size': len(content),
            'lines': len(content.split('\n')),
            'depth': depth,
            'imports': self._extract_imports(content),
            'exports': self._extract_exports(content),
            'functions': self._extract_functions(content),
            'classes': self._extract_classes(content),
            'dependencies': set(),
            'dependents': set(),
            'last_modified': datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),
            'status': FileStatus.ACTIVE,  # Ğ‘ÑƒĞ´Ğµ Ğ¾Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾ Ğ¿Ñ–Ğ·Ğ½Ñ–ÑˆĞµ
        }
        
        # Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ñ–
        for import_path in file_info['imports']:
            dep_path = self._resolve_import(file_path, import_path)
            if dep_path:
                try:
                    dep_key = str(dep_path.relative_to(self.project_root))
                    file_info['dependencies'].add(dep_key)
                    self.dependencies[file_key].add(dep_key)
                    self.reverse_dependencies[dep_key].add(file_key)
                except ValueError:
                    # Ğ—Ğ°Ğ»ĞµĞ¶Ğ½Ñ–ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ·Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ¼, Ñ–Ğ³Ğ½Ğ¾Ñ€ÑƒÑ”Ğ¼Ğ¾
                    pass
        
        # ĞšĞµÑˆÑƒÑ”Ğ¼Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚
        self.files_cache[file_key] = file_info
        
        # Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·ÑƒÑ”Ğ¼Ğ¾ Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ñ–
        if depth < max_depth:
            for dep_key in file_info['dependencies']:
                dep_path = self.project_root / dep_key
                if dep_path.exists():
                    self._analyze_file(dep_path, depth + 1, max_depth)
        
        return file_info
    
    def _extract_imports(self, content: str) -> List[str]:
        """Ğ’Ğ¸Ñ‚ÑĞ³Ñ‚Ğ¸ Ñ–Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸ Ğ· Ñ„Ğ°Ğ¹Ğ»Ñƒ"""
        imports = []
        
        # ES6 imports
        import_pattern = r"import\s+(?:.*?)\s+from\s+['\"]([^'\"]+)['\"]"
        imports.extend(re.findall(import_pattern, content))
        
        # CommonJS requires
        require_pattern = r"require\s*\(\s*['\"]([^'\"]+)['\"]\s*\)"
        imports.extend(re.findall(require_pattern, content))
        
        return imports
    
    def _extract_exports(self, content: str) -> List[str]:
        """Ğ’Ğ¸Ñ‚ÑĞ³Ñ‚Ğ¸ ĞµĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸ Ğ· Ñ„Ğ°Ğ¹Ğ»Ñƒ"""
        exports = []
        
        # ES6 exports
        export_pattern = r"export\s+(?:default\s+)?(?:class|function|const|let|var)\s+(\w+)"
        exports.extend(re.findall(export_pattern, content))
        
        # Named exports
        named_export_pattern = r"export\s+{\s*([^}]+)\s*}"
        matches = re.findall(named_export_pattern, content)
        for match in matches:
            exports.extend([e.strip().split()[0] for e in match.split(',')])
        
        return exports
    
    def _extract_functions(self, content: str) -> List[Dict[str, Any]]:
        """Ğ’Ğ¸Ñ‚ÑĞ³Ñ‚Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ— Ğ· Ñ„Ğ°Ğ¹Ğ»Ñƒ"""
        functions = []
        
        # Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ—
        func_pattern = r"(?:async\s+)?(?:function|const|let)\s+(\w+)\s*(?:\(|=)"
        matches = re.finditer(func_pattern, content)
        
        for match in matches:
            func_name = match.group(1)
            # Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ñ€ÑĞ´Ğ¾Ğº Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ—
            line_num = content[:match.start()].count('\n') + 1
            functions.append({
                'name': func_name,
                'line': line_num,
                'type': 'function'
            })
        
        return functions
    
    def _extract_classes(self, content: str) -> List[Dict[str, Any]]:
        """Ğ’Ğ¸Ñ‚ÑĞ³Ñ‚Ğ¸ ĞºĞ»Ğ°ÑĞ¸ Ğ· Ñ„Ğ°Ğ¹Ğ»Ñƒ"""
        classes = []
        
        # Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ ĞºĞ»Ğ°ÑĞ¸
        class_pattern = r"class\s+(\w+)(?:\s+extends\s+(\w+))?"
        matches = re.finditer(class_pattern, content)
        
        for match in matches:
            class_name = match.group(1)
            extends = match.group(2)
            line_num = content[:match.start()].count('\n') + 1
            classes.append({
                'name': class_name,
                'extends': extends,
                'line': line_num,
                'type': 'class'
            })
        
        return classes
    
    def _resolve_import(self, from_file: Path, import_path: str) -> Optional[Path]:
        """Ğ Ğ¾Ğ·Ğ²'ÑĞ·Ğ°Ñ‚Ğ¸ Ñ–Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ Ğ´Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ñƒ"""
        # Ğ’Ğ¸Ğ´Ğ°Ğ»ÑÑ”Ğ¼Ğ¾ Ñ€Ğ¾Ğ·ÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ ÑĞºÑ‰Ğ¾ Ñ”
        if import_path.startswith('.'):
            # Ğ’Ñ–Ğ´Ğ½Ğ¾ÑĞ½Ğ¸Ğ¹ Ñ–Ğ¼Ğ¿Ğ¾Ñ€Ñ‚
            resolved = (from_file.parent / import_path).resolve()
            
            # ĞŸĞµÑ€ĞµĞ²Ñ–Ñ€ÑÑ”Ğ¼Ğ¾ Ñ€Ñ–Ğ·Ğ½Ñ– Ñ€Ğ¾Ğ·ÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ
            for ext in ['.js', '.ts', '/index.js', '/index.ts']:
                test_path = Path(str(resolved) + ext) if not resolved.suffix else resolved
                if test_path.exists():
                    return test_path
        else:
            # ĞĞ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¸Ğ¹ Ñ–Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ Ğ°Ğ±Ğ¾ node_modules
            # Ğ¨ÑƒĞºĞ°Ñ”Ğ¼Ğ¾ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñ–
            for possible_path in self.project_root.rglob(f"{import_path}*"):
                if possible_path.is_file() and possible_path.suffix in ['.js', '.ts']:
                    return possible_path
        
        return None
    
    def _determine_file_status(self):
        """Ğ’Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚Ğ¸ ÑÑ‚Ğ°Ñ‚ÑƒÑ ĞºĞ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ñƒ"""
        for file_key, file_info in self.files_cache.items():
            # Ğ¯ĞºÑ‰Ğ¾ Ñ„Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ¼Ğ°Ñ” Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¸Ñ…, Ğ¼Ğ¾Ğ¶Ğ»Ğ¸Ğ²Ğ¾ Ğ²Ñ–Ğ½ Ğ½Ğµ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ”Ñ‚ÑŒÑÑ
            if not self.reverse_dependencies.get(file_key):
                self.file_status[file_key] = FileStatus.UNUSED
            # Ğ¯ĞºÑ‰Ğ¾ Ñ„Ğ°Ğ¹Ğ» Ğ¼Ğ°Ñ” Ğ±Ğ°Ğ³Ğ°Ñ‚Ğ¾ Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¸Ñ…, Ğ²Ñ–Ğ½ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¹
            elif len(self.reverse_dependencies.get(file_key, [])) > 2:
                self.file_status[file_key] = FileStatus.ACTIVE
            # Ğ¯ĞºÑ‰Ğ¾ Ñ„Ğ°Ğ¹Ğ» Ğ¼Ğ°Ñ” Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ñ–, Ğ²Ñ–Ğ½ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¹
            elif file_info['dependencies']:
                self.file_status[file_key] = FileStatus.ACTIVE
            else:
                self.file_status[file_key] = FileStatus.ACTIVE
    
    def _build_architecture_map(self, max_depth: int) -> Dict[str, Any]:
        """ĞŸĞ¾Ğ±ÑƒĞ´ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñƒ ĞºĞ°Ñ€Ñ‚Ñƒ"""
        return {
            'timestamp': datetime.now().isoformat(),
            'project_root': str(self.project_root),
            'analysis_root': str(self.analysis_root),
            'max_depth': max_depth,
            'files': self._serialize_files(),
            'dependencies': self._serialize_dependencies(),
            'statistics': self._calculate_statistics(),
            'health_score': self._calculate_health_score(),
        }
    
    def _serialize_files(self) -> Dict[str, Any]:
        """Ğ¡ĞµÑ€Ñ–Ğ°Ğ»Ñ–Ğ·ÑƒĞ²Ğ°Ñ‚Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ¸ Ğ´Ğ»Ñ JSON"""
        result = {}
        for file_key, file_info in self.files_cache.items():
            result[file_key] = {
                'size': file_info['size'],
                'lines': file_info['lines'],
                'depth': file_info['depth'],
                'status': self.file_status.get(file_key, FileStatus.ACTIVE),
                'imports_count': len(file_info['imports']),
                'exports_count': len(file_info['exports']),
                'functions_count': len(file_info['functions']),
                'classes_count': len(file_info['classes']),
                'dependencies_count': len(file_info['dependencies']),
                'dependents_count': len(self.reverse_dependencies.get(file_key, [])),
                'last_modified': file_info['last_modified'],
                'functions': file_info['functions'],
                'classes': file_info['classes'],
            }
        return result
    
    def _serialize_dependencies(self) -> Dict[str, List[str]]:
        """Ğ¡ĞµÑ€Ñ–Ğ°Ğ»Ñ–Ğ·ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ñ–"""
        result = {}
        for file_key, deps in self.dependencies.items():
            result[file_key] = sorted(list(deps))
        return result
    
    def _calculate_statistics(self) -> Dict[str, Any]:
        """Ğ Ğ¾Ğ·Ñ€Ğ°Ñ…ÑƒĞ²Ğ°Ñ‚Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ"""
        total_files = len(self.files_cache)
        total_lines = sum(f['lines'] for f in self.files_cache.values())
        total_size = sum(f['size'] for f in self.files_cache.values())
        
        status_counts = defaultdict(int)
        for status in self.file_status.values():
            status_counts[status] += 1
        
        return {
            'total_files': total_files,
            'total_lines': total_lines,
            'total_size': total_size,
            'average_lines_per_file': total_lines // total_files if total_files > 0 else 0,
            'status_distribution': dict(status_counts),
            'unused_files': sum(1 for s in self.file_status.values() if s == FileStatus.UNUSED),
            'active_files': sum(1 for s in self.file_status.values() if s == FileStatus.ACTIVE),
        }
    
    def _calculate_health_score(self) -> Dict[str, Any]:
        """Ğ Ğ¾Ğ·Ñ€Ğ°Ñ…ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ¾Ñ†Ñ–Ğ½ĞºÑƒ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²'Ñ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¸"""
        stats = self._calculate_statistics()
        
        # ĞÑ†Ñ–Ğ½ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ñ– Ñ€Ñ–Ğ·Ğ½Ğ¸Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ–Ğ²
        unused_ratio = stats['unused_files'] / stats['total_files'] if stats['total_files'] > 0 else 0
        avg_deps = sum(len(d) for d in self.dependencies.values()) / len(self.dependencies) if self.dependencies else 0
        
        # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ° Ğ¾Ñ†Ñ–Ğ½ĞºĞ° 100
        score = 100
        
        # Ğ¨Ñ‚Ñ€Ğ°Ñ„ Ğ·Ğ° Ğ½ĞµĞ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ²Ğ°Ğ½Ñ– Ñ„Ğ°Ğ¹Ğ»Ğ¸
        score -= unused_ratio * 20
        
        # Ğ¨Ñ‚Ñ€Ğ°Ñ„ Ğ·Ğ° Ğ²Ğ¸ÑĞ¾ĞºÑƒ Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ñ–ÑÑ‚ÑŒ
        if avg_deps > 5:
            score -= (avg_deps - 5) * 2
        
        # Ğ‘Ğ¾Ğ½ÑƒÑ Ğ·Ğ° Ğ´Ğ¾Ğ±Ñ€Ñƒ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ–ÑÑ‚ÑŒ
        if stats['total_files'] > 10:
            score += 10
        
        return {
            'score': max(0, min(100, score)),
            'unused_ratio': unused_ratio,
            'average_dependencies': avg_deps,
            'modularity': 'good' if stats['total_files'] > 10 else 'fair',
        }
    
    def export_architecture(self, output_path: Path) -> None:
        """Ğ•ĞºÑĞ¿Ğ¾Ñ€Ñ‚ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ² JSON"""
        architecture = self.analyze_architecture()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(architecture, f, indent=2, default=str)
        
        print(f"âœ… ĞÑ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ĞµĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ² {output_path}")
    
    def generate_architecture_report(self) -> str:
        """Ğ“ĞµĞ½ĞµÑ€ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ·Ğ²Ñ–Ñ‚ Ğ¿Ñ€Ğ¾ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ"""
        architecture = self.analyze_architecture()
        stats = architecture['statistics']
        health = architecture['health_score']
        
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           ĞĞ Ğ¥Ğ†Ğ¢Ğ•ĞšĞ¢Ğ£Ğ ĞĞ ĞšĞĞ Ğ¢Ğ Ğ¡Ğ˜Ğ¡Ğ¢Ğ•ĞœĞ˜ WORKFLOW                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ:
   â€¢ Ğ’ÑÑŒĞ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ²: {stats['total_files']}
   â€¢ ĞĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸Ñ… Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ²: {stats['active_files']}
   â€¢ ĞĞµĞ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ²Ğ°Ğ½Ğ¸Ñ… Ñ„Ğ°Ğ¹Ğ»Ñ–Ğ²: {stats['unused_files']}
   â€¢ Ğ’ÑÑŒĞ¾Ğ³Ğ¾ Ñ€ÑĞ´ĞºÑ–Ğ² ĞºĞ¾Ğ´Ñƒ: {stats['total_lines']}
   â€¢ Ğ¡ĞµÑ€ĞµĞ´Ğ½Ñ Ğ´Ğ¾Ğ²Ğ¶Ğ¸Ğ½Ğ° Ñ„Ğ°Ğ¹Ğ»Ñƒ: {stats['average_lines_per_file']} Ñ€ÑĞ´ĞºÑ–Ğ²
   â€¢ Ğ—Ğ°Ğ³Ğ°Ğ»ÑŒĞ½Ğ¸Ğ¹ Ñ€Ğ¾Ğ·Ğ¼Ñ–Ñ€: {stats['total_size']} Ğ±Ğ°Ğ¹Ñ‚

ğŸ¥ Ğ—Ğ”ĞĞ ĞĞ’'Ğ¯ ĞĞ Ğ¥Ğ†Ğ¢Ğ•ĞšĞ¢Ğ£Ğ Ğ˜:
   â€¢ ĞÑ†Ñ–Ğ½ĞºĞ°: {health['score']:.1f}/100
   â€¢ ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ–ÑÑ‚ÑŒ: {health['modularity']}
   â€¢ ĞšĞ¾ĞµÑ„Ñ–Ñ†Ñ–Ñ”Ğ½Ñ‚ Ğ½ĞµĞ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ°Ğ½Ğ½Ñ: {health['unused_ratio']:.1%}
   â€¢ Ğ¡ĞµÑ€ĞµĞ´Ğ½Ñ Ğ·Ğ°Ğ»ĞµĞ¶Ğ½Ñ–ÑÑ‚ÑŒ: {health['average_dependencies']:.1f}

ğŸ“ Ğ ĞĞ—ĞŸĞĞ”Ğ†Ğ› Ğ¡Ğ¢ĞĞ¢Ğ£Ğ¡Ğ†Ğ’:
"""
        for status, count in stats['status_distribution'].items():
            report += f"   â€¢ {status}: {count}\n"
        
        return report


if __name__ == '__main__':
    project_root = Path(__file__).parent.parent
    mapper = ArchitectureMapper(project_root)
    
    # Ğ“ĞµĞ½ĞµÑ€ÑƒÑ”Ğ¼Ğ¾ Ğ·Ğ²Ñ–Ñ‚
    print(mapper.generate_architecture_report())
    
    # Ğ•ĞºÑĞ¿Ğ¾Ñ€Ñ‚ÑƒÑ”Ğ¼Ğ¾ Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ
    output_path = Path(__file__).parent / 'reports' / 'architecture_map.json'
    output_path.parent.mkdir(parents=True, exist_ok=True)
    mapper.export_architecture(output_path)
