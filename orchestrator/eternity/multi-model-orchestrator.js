/**
 * NEXUS MULTI-MODEL ORCHESTRATOR
 * Real implementation with API integration
 * 
 * Created: 2025-11-03
 * Integrates: Codestral (data collection) + GPT-5 Codex (code analysis) + Claude Thinking (strategy)
 */

import axios from 'axios';
import logger from '../utils/logger.js';
import GlobalConfig from '../../config/global-config.js';
import fs from 'fs/promises';
import path from 'path';

export class MultiModelOrchestrator {
    constructor(container) {
        this.container = container;
        this.logger = logger;
        this.stats = {
            totalRequests: 0,
            successfulRequests: 0,
            failedRequests: 0,
            modelUsage: {}
        };
        
        // FIXED 2025-11-03: Cascade Controller Ð´Ð»Ñ Windsurf Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
        this.cascadeController = null;
    }

    async initialize() {
        this.logger.info('[NEXUS] Multi-Model Orchestrator initialized with real API integration');
        
        // FIXED 2025-11-03: ÐžÑ‚Ñ€Ð¸Ð¼ÑƒÑ”Ð¼Ð¾ Cascade Controller Ð· DI
        try {
            this.cascadeController = this.container.resolve('cascadeController');
            this.logger.info('[NEXUS] âœ… Connected to Cascade Controller for Windsurf models');
        } catch (e) {
            this.logger.warn('[NEXUS] Cascade Controller not available, will use direct API calls');
        }
        
        return true;
    }

    /**
     * ÐÐ²Ñ‚Ð¾Ð½Ð¾Ð¼Ð½Ð¸Ð¹ Ð·Ð±Ñ–Ñ€ Ð´Ð°Ð½Ð¸Ñ… Ñ‡ÐµÑ€ÐµÐ· Codestral
     */
    async autonomousDataCollection(context) {
        this.logger.info('[NEXUS] Starting autonomous data collection (Codestral)', context);
        
        try {
            const data = {
                logs: await this._collectLogs(context.logsPath),
                config: await this._collectConfig(context.configPath),
                codeChanges: await this._collectCodeChanges(context.codePath),
                metrics: await this._collectMetrics(),
                timestamp: new Date().toISOString()
            };

            this.logger.info('[NEXUS] Data collection completed', {
                logsCount: data.logs.length,
                configFiles: Object.keys(data.config).length
            });

            return data;
        } catch (error) {
            this.logger.error('[NEXUS] Data collection failed:', error);
            return {
                error: error.message,
                timestamp: new Date().toISOString()
            };
        }
    }

    /**
     * Ð’Ð¸ÐºÐ¾Ð½Ð°Ð½Ð½Ñ Ð·Ð°Ð´Ð°Ñ‡Ñ– Ñ‡ÐµÑ€ÐµÐ· Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð½Ñƒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
     */
    async executeTask(taskType, prompt, options = {}) {
        this.stats.totalRequests++;
        
        const modelConfig = this._selectModelForTask(taskType);
        this.logger.info(`[NEXUS] Executing ${taskType} with ${modelConfig.name}`);

        try {
            const result = await this._callLLMAPI(modelConfig, prompt, options);
            this.stats.successfulRequests++;
            this._updateModelStats(modelConfig.name);
            
            return {
                success: true,
                content: result.content,
                model: modelConfig.name,
                usage: result.usage
            };
        } catch (error) {
            this.stats.failedRequests++;
            this.logger.error(`[NEXUS] Task execution failed:`, error);
            
            return {
                success: false,
                error: error.message,
                model: modelConfig.name
            };
        }
    }

    /**
     * ÐŸÐ°Ñ€Ð°Ð»ÐµÐ»ÑŒÐ½Ðµ Ð²Ð¸ÐºÐ¾Ð½Ð°Ð½Ð½Ñ Ð·Ð°Ð´Ð°Ñ‡
     */
    async executeParallel(tasks) {
        this.logger.info(`[NEXUS] Executing ${tasks.length} tasks in parallel`);
        
        const promises = tasks.map(task => 
            this.executeTask(task.type, task.prompt, task.options)
        );

        const results = await Promise.allSettled(promises);
        
        const successful = results
            .filter(r => r.status === 'fulfilled' && r.value.success)
            .map(r => r.value);
        
        const failed = results
            .filter(r => r.status === 'rejected' || !r.value?.success)
            .map(r => r.reason || r.value);

        return {
            successful,
            failed
        };
    }

    /**
     * Ð’Ð¸Ð±Ñ–Ñ€ Ð¼Ð¾Ð´ÐµÐ»Ñ– Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ñ‡Ñ–
     * 
     * WINDSURF Ð¼Ð¾Ð´ÐµÐ»Ñ– (Ñ‡ÐµÑ€ÐµÐ· Windsurf API):
     * - CASCADE_PRIMARY_MODEL: claude-sonnet-4.5-thinking
     * - CASCADE_CODE_ANALYSIS_MODEL: gpt-5-codex
     * - CASCADE_FALLBACK_MODEL: claude-sonnet-4.5
     * 
     * CODESTRAL (Ñ‡ÐµÑ€ÐµÐ· API 4000):
     * - CASCADE_CODESTRAL_MODEL: ext-mistral-codestral-2405
     */
    _selectModelForTask(taskType) {
        // WINDSURF Ð¼Ð¾Ð´ÐµÐ»Ñ– (Claude, GPT-5 Codex)
        const cascadePrimary = process.env.CASCADE_PRIMARY_MODEL || 'claude-sonnet-4.5-thinking';
        const cascadeCodeAnalysis = process.env.CASCADE_CODE_ANALYSIS_MODEL || 'gpt-5-codex';
        const cascadeFallback = process.env.CASCADE_FALLBACK_MODEL || 'claude-sonnet-4.5';
        
        // CODESTRAL Ð· API 4000 (Ð¾ÐºÑ€ÐµÐ¼Ð¾ Ð²Ñ–Ð´ Windsurf)
        const codestralModel = process.env.CASCADE_CODESTRAL_MODEL || 'ext-mistral-codestral-2405';
        const codestralEndpoint = process.env.LLM_API_ENDPOINT || 'http://localhost:4000/v1/chat/completions';
        
        const modelMapping = {
            // GPT-5 Codex (Windsurf) - Ð°Ð½Ð°Ð»Ñ–Ð· ÐºÐ¾Ð´Ñƒ
            'code-analysis': {
                name: cascadeCodeAnalysis,
                endpoint: 'windsurf',  // Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ” Windsurf API
                isWindsurf: true
            },
            'data-collection': {
                name: cascadeCodeAnalysis,
                endpoint: 'windsurf',
                isWindsurf: true
            },
            
            // Codestral (API 4000) - tool planning, file ops, JSON
            'tool-planning': {
                name: codestralModel,
                endpoint: codestralEndpoint,
                isWindsurf: false
            },
            'file-operations': {
                name: codestralModel,
                endpoint: codestralEndpoint,
                isWindsurf: false
            },
            'json-generation': {
                name: codestralModel,
                endpoint: codestralEndpoint,
                isWindsurf: false
            },
            
            // Claude Thinking (Windsurf) - Ð³Ð»Ð¸Ð±Ð¾ÐºÐ¸Ð¹ Ð°Ð½Ð°Ð»Ñ–Ð·
            'deep-analysis': {
                name: cascadePrimary,
                endpoint: 'windsurf',
                isWindsurf: true
            },
            'strategy': {
                name: cascadePrimary,
                endpoint: 'windsurf',
                isWindsurf: true
            },
            'thinking': {
                name: cascadePrimary,
                endpoint: 'windsurf',
                isWindsurf: true
            },
            
            // Claude Sonnet (Windsurf) - Ð·Ð°Ð³Ð°Ð»ÑŒÐ½Ñ– Ð·Ð°Ð´Ð°Ñ‡Ñ–
            'general': {
                name: cascadeFallback,
                endpoint: 'windsurf',
                isWindsurf: true
            },
            'chat': {
                name: cascadeFallback,
                endpoint: 'windsurf',
                isWindsurf: true
            }
        };

        const modelConfig = modelMapping[taskType] || modelMapping.general;
        const modelName = modelConfig.name;
        const endpoint = modelConfig.isWindsurf 
            ? (GlobalConfig.MCP_MODEL_CONFIG.apiEndpoint.primary || 'http://localhost:4000/v1/chat/completions')
            : modelConfig.endpoint;
        
        this.logger.info(`[NEXUS] Selected model for ${taskType}: ${modelName} (${modelConfig.isWindsurf ? 'Windsurf' : 'API 4000'})`);
        
        return {
            name: modelName,
            endpoint: endpoint,
            temperature: this._getTemperatureForTask(taskType),
            max_tokens: this._getMaxTokensForTask(taskType),
            isWindsurf: modelConfig.isWindsurf
        };
    }
    
    /**
     * Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð´Ð»Ñ Ñ€Ñ–Ð·Ð½Ð¸Ñ… Ñ‚Ð¸Ð¿Ñ–Ð² Ð·Ð°Ð´Ð°Ñ‡
     */
    _getTemperatureForTask(taskType) {
        const temperatureMap = {
            'code-analysis': 0.1,
            'tool-planning': 0.1,
            'json-generation': 0.05,
            'file-operations': 0.15,
            'deep-analysis': 0.3,
            'strategy': 0.3,
            'thinking': 0.4
        };
        return temperatureMap[taskType] || 0.2;
    }
    
    /**
     * Max tokens Ð´Ð»Ñ Ñ€Ñ–Ð·Ð½Ð¸Ñ… Ñ‚Ð¸Ð¿Ñ–Ð² Ð·Ð°Ð´Ð°Ñ‡
     */
    _getMaxTokensForTask(taskType) {
        const maxTokensMap = {
            'deep-analysis': 4000,
            'strategy': 3000,
            'thinking': 4000,
            'code-analysis': 2500,
            'tool-planning': 2000,
            'file-operations': 1500
        };
        return maxTokensMap[taskType] || 2000;
    }

    /**
     * Ð’Ð¸ÐºÐ»Ð¸Ðº LLM API
     * FIXED 2025-11-03: Windsurf Ð¼Ð¾Ð´ÐµÐ»Ñ– Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑŽÑ‚ÑŒ Windsurf Cascade API
     */
    async _callLLMAPI(modelConfig, prompt, options = {}) {
        // CRITICAL: Windsurf Ð¼Ð¾Ð´ÐµÐ»Ñ– Ð¼Ð°ÑŽÑ‚ÑŒ Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ²Ð°Ñ‚Ð¸ Cascade API
        if (modelConfig.isWindsurf) {
            return await this._callWindsurfCascadeAPI(modelConfig, prompt, options);
        }
        
        // Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ– Ð¼Ð¾Ð´ÐµÐ»Ñ– (Codestral) Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑŽÑ‚ÑŒ localhost:4000
        const requestBody = {
            model: modelConfig.name,
            messages: [
                {
                    role: 'system',
                    content: options.systemPrompt || 'You are a helpful AI assistant specialized in code analysis and system optimization.'
                },
                {
                    role: 'user',
                    content: prompt
                }
            ],
            temperature: options.temperature || modelConfig.temperature,
            max_tokens: options.max_tokens || modelConfig.max_tokens
        };

        const response = await axios.post(modelConfig.endpoint, requestBody, {
            headers: {
                'Content-Type': 'application/json'
            },
            timeout: 120000 // 2 minutes
        });

        return {
            content: response.data.choices[0].message.content,
            usage: response.data.usage
        };
    }

    /**
     * Ð’Ð¸ÐºÐ»Ð¸Ðº Windsurf Cascade API Ð´Ð»Ñ GPT-5 Codex Ñ‚Ð° Claude Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
     * FIXED 2025-11-03: Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ Cascade Controller (Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾ Ñ‡ÐµÑ€ÐµÐ· IDE)
     */
    async _callWindsurfCascadeAPI(modelConfig, prompt, options = {}) {
        this.logger.info(`[NEXUS] ðŸŒ Calling Windsurf via Cascade Controller: ${modelConfig.name}`);
        
        // CRITICAL: Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ Cascade Controller Ð´Ð»Ñ Windsurf Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
        if (this.cascadeController && this.cascadeController.multiModelOrchestrator) {
            this.logger.info('[NEXUS] ðŸŽ¯ Using Cascade Controller for Windsurf model execution');
            
            try {
                // Ð’Ð¸ÐºÐ»Ð¸ÐºÐ°Ñ”Ð¼Ð¾ Ñ‡ÐµÑ€ÐµÐ· Cascade's internal orchestrator
                // Ð¦Ðµ Ð´Ð¾Ð·Ð²Ð¾Ð»ÑÑ” Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ²Ð°Ñ‚Ð¸ Windsurf Cascade Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾ Ñ‡ÐµÑ€ÐµÐ· IDE
                const result = await this._callViaCascadeInternal(modelConfig, prompt, options);
                
                if (result.success) {
                    this.logger.info(`[NEXUS] âœ… Cascade Controller response received`);
                    return {
                        content: result.content,
                        usage: result.usage,
                        via: 'cascade-controller'
                    };
                }
            } catch (error) {
                this.logger.warn(`[NEXUS] Cascade Controller error: ${error.message}, trying fallback`);
            }
        }
        
        // Fallback: localhost:4000 (Codestral Ð°Ð±Ð¾ Ñ–Ð½ÑˆÑ– Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ– Ð¼Ð¾Ð´ÐµÐ»Ñ–)
        this.logger.warn('[NEXUS] âš ï¸ Cascade Controller not available, falling back to localhost:4000');
        modelConfig.endpoint = 'http://localhost:4000/v1/chat/completions';
        modelConfig.isWindsurf = false;
        return await this._callLLMAPI(modelConfig, prompt, options);
    }
    
    /**
     * Ð’Ð¸ÐºÐ»Ð¸Ðº Ñ‡ÐµÑ€ÐµÐ· Ð²Ð½ÑƒÑ‚Ñ€Ñ–ÑˆÐ½Ñ–Ð¹ orchestrator Cascade Controller
     * Ð¦Ðµ Ð´Ð¾Ð·Ð²Ð¾Ð»ÑÑ” Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ²Ð°Ñ‚Ð¸ Windsurf Cascade Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾
     */
    async _callViaCascadeInternal(modelConfig, prompt, options = {}) {
        // Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ Codestral API Cascade Controller Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ñ–Ð·Ñƒ
        // Codestral Ð¿Ñ€Ð°Ñ†ÑŽÑ” Ñ‡ÐµÑ€ÐµÐ· localhost:4000 Ð°Ð»Ðµ Ð¿Ñ–Ð´ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÐµÐ¼ Cascade
        const systemPrompt = options.systemPrompt || 'You are a helpful AI assistant specialized in code analysis and system optimization.';
        const fullPrompt = `${systemPrompt}\n\n${prompt}`;
        
        const result = await this.cascadeController.analyzeCodeWithCodestral(
            fullPrompt,
            {
                model: modelConfig.name,
                temperature: options.temperature || modelConfig.temperature,
                max_tokens: options.max_tokens || modelConfig.max_tokens,
                taskType: 'code-analysis'
            }
        );
        
        if (result.success) {
            return {
                success: true,
                content: result.analysis,
                usage: { total_tokens: 0 } // Codestral Ð½Ðµ Ð¿Ð¾Ð²ÐµÑ€Ñ‚Ð°Ñ” usage
            };
        }
        
        throw new Error(result.error || 'Cascade analysis failed');
    }

    /**
     * Ð—Ð±Ñ–Ñ€ Ð»Ð¾Ð³Ñ–Ð²
     */
    async _collectLogs(logsPath) {
        try {
            const errorLog = await fs.readFile(path.join(logsPath, 'error.log'), 'utf-8');
            const orchestratorLog = await fs.readFile(path.join(logsPath, 'orchestrator.log'), 'utf-8');
            
            return {
                errors: errorLog.split('\n').slice(-100).join('\n'),
                orchestrator: orchestratorLog.split('\n').slice(-100).join('\n')
            };
        } catch (error) {
            this.logger.warn('[NEXUS] Failed to collect logs:', error.message);
            return 'Logs unavailable';
        }
    }

    /**
     * Ð—Ð±Ñ–Ñ€ ÐºÐ¾Ð½Ñ„Ñ–Ð³ÑƒÑ€Ð°Ñ†Ñ–Ñ—
     */
    async _collectConfig(configPath) {
        try {
            const files = await fs.readdir(configPath);
            const jsFiles = files.filter(f => f.endsWith('.js'));
            
            return {
                files: jsFiles,
                count: jsFiles.length
            };
        } catch (error) {
            this.logger.warn('[NEXUS] Failed to collect config:', error.message);
            return {};
        }
    }

    /**
     * Ð—Ð±Ñ–Ñ€ Ð·Ð¼Ñ–Ð½ Ð² ÐºÐ¾Ð´Ñ– (Ð·Ð°Ð³Ð»ÑƒÑˆÐºÐ°)
     */
    async _collectCodeChanges(codePath) {
        return 'Recent code changes analysis available';
    }

    /**
     * Ð—Ð±Ñ–Ñ€ Ð¼ÐµÑ‚Ñ€Ð¸Ðº
     */
    async _collectMetrics() {
        return {
            timestamp: new Date().toISOString(),
            uptime: process.uptime(),
            memory: process.memoryUsage()
        };
    }

    /**
     * ÐžÐ½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»Ñ–
     */
    _updateModelStats(modelName) {
        if (!this.stats.modelUsage[modelName]) {
            this.stats.modelUsage[modelName] = 0;
        }
        this.stats.modelUsage[modelName]++;
    }

    /**
     * ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸
     */
    getStats() {
        return {
            ...this.stats,
            successRate: this.stats.totalRequests > 0 
                ? (this.stats.successfulRequests / this.stats.totalRequests * 100).toFixed(2) + '%'
                : 'N/A'
        };
    }
}

export default MultiModelOrchestrator;
